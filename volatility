import pandas as pd
import numpy as np

def get_constant_maturity_iv(tickers, start_date, end_date, maturity=30/365.25):
    """
    Generates a time series of constant maturity ATM implied volatility for multiple tickers.

    Args:
        tickers (list): List of stock tickers.
        start_date (str): Start date in 'YYYY-MM-DD' format.
        end_date (str): End date in 'YYYY-MM-DD' format.
        maturity (float): Desired constant maturity in years (e.g., 30 days = 30/365.25).

    Returns:
        pd.DataFrame: A DataFrame with dates as the index and IV time series as columns.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    all_ivs = {}

    for ticker in tickers:
        ticker_ivs = []
        for date in dates:
            try:
                surface = get_iv_surface(ticker, date) # User's provided function
                expiries = surface['Expiry'].values
                atm_vols = surface['1.0'].values
                
                # Linear interpolation to find the constant maturity IV
                constant_iv = np.interp(maturity, expiries, atm_vols)
                ticker_ivs.append(constant_iv)
            except Exception as e:
                ticker_ivs.append(np.nan) # Handle cases where data is not available
        all_ivs[ticker] = ticker_ivs
        
    df_iv = pd.DataFrame(all_ivs, index=dates)
    df_iv.dropna(how='all', inplace=True)
    return df_iv

# Example Usage:
# spy_components = spy_weights['CMDB Ticker'].tolist()
# iv_timeseries = get_constant_maturity_iv(spy_components, '2022-01-01', '2023-12-31')


def analyze_iv_comovement(df_iv, benchmark_ticker, stress_threshold=0.02):
    """
    Analyzes the co-movement of implied volatilities against a benchmark.

    Args:
        df_iv (pd.DataFrame): Time series of implied volatilities.
        benchmark_ticker (str): The ticker to use as the benchmark (e.g., 'SPY-US').
        stress_threshold (float): The daily percentage drop in benchmark to define a stress day.

    Returns:
        dict: A dictionary containing correlation and beta matrices for all, stress, and regular periods.
    """
    iv_returns = df_iv.pct_change().dropna()
    benchmark_returns = iv_returns[benchmark_ticker]
    
    # Identify stress periods
    stress_days = benchmark_returns < -stress_threshold
    regular_days = ~stress_days

    # Calculate correlation
    corr_all = iv_returns.corr()
    corr_stress = iv_returns[stress_days].corr()
    corr_regular = iv_returns[regular_days].corr()

    # Calculate scaling effect (beta)
    betas = iv_returns.apply(lambda x: np.polyfit(benchmark_returns, x, 1)[0])
    
    return {
        "correlation_all": corr_all,
        "correlation_stress": corr_stress,
        "correlation_regular": corr_regular,
        "scaling_betas": betas
    }

# Example Usage:
# comovement_results = analyze_iv_comovement(iv_timeseries, 'SPY-US')
# print("Scaling Betas:\n", comovement_results['scaling_betas'])


from arch import arch_model

def calculate_vol_of_vol(df_iv):
    """
    Calculates the volatility of implied volatility using three different methods.

    Args:
        df_iv (pd.DataFrame): Time series of implied volatilities.

    Returns:
        dict: A dictionary of DataFrames for each vol-of-vol calculation method.
    """
    iv_returns = df_iv.pct_change().dropna()
    
    # 1. 21-Day Rolling Window
    vol_rolling = iv_returns.rolling(window=21).std() * np.sqrt(252)

    # 2. EWMA
    vol_ewma = iv_returns.ewm(span=21, adjust=False).std() * np.sqrt(252)
    
    # 3. EGARCH(1,1)
    def fit_egarch(series):
        model = arch_model(series * 100, p=1, o=1, q=1, vol='EGARCH')
        res = model.fit(disp='off')
        return res.conditional_volatility / 100
        
    vol_egarch = iv_returns.apply(fit_egarch, axis=0) * np.sqrt(252)

    return {
        "rolling_21d": vol_rolling,
        "ewma_21d": vol_ewma,
        "egarch_1_1": vol_egarch
    }

# Example Usage:
# vol_of_vol_results = calculate_vol_of_vol(iv_timeseries)
# print("EWMA Vol of Vol:\n", vol_of_vol_results['ewma_21d'].tail())



from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

def verify_power_law_decay(ticker, valuation_date):
    """
    Fits a power law to the IV term structure using L2 regularized regression.
    The model is IV = c + b1*T^(-0.5) + b2*T^(-1.0), fit via log-log regression.

    Args:
        ticker (str): The stock ticker.
        valuation_date (str): The valuation date 'YYYY-MM-DD'.

    Returns:
        dict: A dictionary with the regression model and its parameters.
    """
    surface = get_iv_surface(ticker, pd.to_datetime(valuation_date))
    surface = surface.loc[surface['Expiry'] > 0]
    
    X = np.log(surface[['Expiry']].values)
    y = np.log(surface['1.0'].values)
    
    # Using Ridge (L2 regularization) on a log-log plot to find the power
    model = make_pipeline(PolynomialFeatures(1), Ridge(alpha=1.0))
    model.fit(X, y)
    
    # The coefficient of the log(T) term is the power 'alpha'
    # log(IV) = log(k) + alpha * log(T)
    alpha = model.named_steps['ridge'].coef_[1]
    
    return {
        "model": model,
        "power_law_exponent (alpha)": alpha
    }

# Example Usage:
# power_law_fit = verify_power_law_decay('AAPL-UQ', '2023-06-30')
# print(f"Fitted Power Law Exponent: {power_law_fit['power_law_exponent (alpha)']:.4f}")



def backtest_beta_hedging(portfolio_tickers, hedge_instrument, df_iv, betas):
    """
    Runs a simple backtest for a beta-weighted Vega hedge.

    Args:
        portfolio_tickers (list): List of tickers in the long portfolio.
        hedge_instrument (str): The ticker for the hedging instrument (e.g., 'SPY-US').
        df_iv (pd.DataFrame): Time series of implied volatilities.
        betas (pd.Series): Series of scaling betas against the hedge instrument.

    Returns:
        pd.DataFrame: A DataFrame showing the unhedged vs. hedged portfolio IV changes.
    """
    iv_returns = df_iv.pct_change().dropna()

    # Assume an equally-weighted portfolio (long 1 unit of IV in each stock)
    portfolio_iv_returns = iv_returns[portfolio_tickers].mean(axis=1)

    # Calculate the portfolio's aggregate beta
    portfolio_beta = betas[portfolio_tickers].mean()
    
    # The optimal hedge ratio is the portfolio's beta
    # We short 'portfolio_beta' units of the hedge instrument's IV
    hedge_iv_returns = portfolio_beta * iv_returns[hedge_instrument]
    
    # Calculate hedged portfolio returns
    hedged_portfolio_iv_returns = portfolio_iv_returns - hedge_iv_returns
    
    # Create results DataFrame
    results = pd.DataFrame({
        'unhedged_iv_change': portfolio_iv_returns,
        'hedged_iv_change': hedged_portfolio_iv_returns
    })
    
    print(f"Unhedged Volatility: {results['unhedged_iv_change'].std() * 100:.4f}%")
    print(f"Hedged Volatility:   {results['hedged_iv_change'].std() * 100:.4f}%")
    
    return results

# Example Usage:
# Assume we ran Module 2 and got the betas
# betas = comovement_results['scaling_betas']
# my_portfolio = ['AAPL-UQ', 'MSFT-US', 'NVDA-US'] # Our proxy portfolio
# hedge_results = backtest_beta_hedging(my_portfolio, 'SPY-US', iv_timeseries, betas)


import pandas as pd
import numpy as np

def store_vol_surfaces_to_hdf(spy_weights_df, start_date, end_date, output_path='vol_surfaces.h5'):
    """
    Fetches and stores IV surfaces in an HDF5 file with string-formatted columns.
    """
    top_50_tickers = spy_weights_df.nlargest(50, 'ID().WEIGHTS')['CMDB Ticker'].tolist()
    all_tickers = list(set(top_50_tickers + ['SPY-UP', 'SPX']))
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))

    with pd.HDFStore(output_path, mode='w', complevel=9, complib='blosc') as store:
        for ticker in all_tickers:
            for date in dates:
                # This is your provided function to get the raw surface
                surface_df = get_iv_surface(ticker, date)

                # Convert numeric columns to valid string names (e.g., 1.0 -> 'm_1_0')
                rename_map = {c: f"m_{str(c).replace('.', '_')}" for c in surface_df.columns if isinstance(c, (int, float))}
                surface_df.rename(columns=rename_map, inplace=True)
                
                # Create a safe key for HDF5 storage
                key = f'/{ticker.replace("-", "_").replace(".", "")}/{date.strftime("d%Y%m%d")}'
                
                # Store the DataFrame
                store.put(key, surface_df, format='table')
                
                # Store metadata
                store.get_storer(key).attrs.metadata = {
                    'spot': surface_df.spot,
                    'valuation_date': surface_df.valuation_date
                }

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity=30/365.25):
    """
    Calculates fixed-strike IV changes using the corrected HDF5 file.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    iv_changes = {}

    with pd.HDFStore(db_path, mode='r') as store:
        for ticker in tickers:
            ticker_key_name = ticker.replace("-", "_").replace(".", "")
            prev_atm_iv = np.nan
            prev_spot = np.nan
            daily_changes = []

            for date in dates:
                key = f'/{ticker_key_name}/{date.strftime("d%Y%m%d")}'
                if key not in store.keys():
                    daily_changes.append(np.nan)
                    continue

                surface = store[key]
                metadata = store.get_storer(key).attrs.metadata
                spot_today = metadata['spot']
                
                iv_cols = [c for c in surface.columns if c.startswith('m_')]
                moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
                
                # Sort them to ensure they are in ascending order for interpolation
                sorted_indices = np.argsort(moneyness_levels)
                moneyness_levels = np.array(moneyness_levels)[sorted_indices]
                iv_surface_values = surface[np.array(iv_cols)[sorted_indices]].values
                
                expiries = surface['Expiry'].values
                constant_maturity_vols = np.apply_along_axis(
                    lambda vol_col: np.interp(maturity, expiries, vol_col), 0, iv_surface_values
                )

                if not np.isnan(prev_spot):
                    moneyness_of_prev_strike = prev_spot / spot_today
                    iv_of_prev_strike_today = np.interp(
                        moneyness_of_prev_strike, moneyness_levels, constant_maturity_vols
                    )
                    change = iv_of_prev_strike_today - prev_atm_iv
                    daily_changes.append(change)
                else:
                    daily_changes.append(np.nan)

                current_atm_iv = np.interp(1.0, moneyness_levels, constant_maturity_vols)
                prev_atm_iv = current_atm_iv
                prev_spot = spot_today

            iv_changes[ticker] = daily_changes

    results_df = pd.DataFrame(iv_changes, index=dates)
    return results_df.dropna(how='all')

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity=30/365.25):
    """
    Calculates fixed-strike IV changes using the corrected HDF5 file.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    iv_changes = {}

    with pd.HDFStore(db_path, mode='r') as store:
        for ticker in tickers:
            ticker_key_name = ticker.replace("-", "_").replace(".", "")
            prev_atm_iv = np.nan
            prev_spot = np.nan
            daily_changes = []

            for date in dates:
                key = f'/{ticker_key_name}/{date.strftime("d%Y%m%d")}'
                if key not in store.keys():
                    daily_changes.append(np.nan)
                    continue

                surface = store[key]
                metadata = store.get_storer(key).attrs.metadata
                spot_today = metadata['spot']
                
                iv_cols = [c for c in surface.columns if c.startswith('m_')]
                moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
                
                # Sort them to ensure they are in ascending order for interpolation
                sorted_indices = np.argsort(moneyness_levels)
                moneyness_levels = np.array(moneyness_levels)[sorted_indices]
                iv_surface_values = surface[np.array(iv_cols)[sorted_indices]].values
                
                expiries = surface['Expiry'].values
                constant_maturity_vols = np.apply_along_axis(
                    lambda vol_col: np.interp(maturity, expiries, vol_col), 0, iv_surface_values
                )

                if not np.isnan(prev_spot):
                    moneyness_of_prev_strike = prev_spot / spot_today
                    iv_of_prev_strike_today = np.interp(
                        moneyness_of_prev_strike, moneyness_levels, constant_maturity_vols
                    )
                    change = iv_of_prev_strike_today - prev_atm_iv
                    daily_changes.append(change)
                else:
                    daily_changes.append(np.nan)

                current_atm_iv = np.interp(1.0, moneyness_levels, constant_maturity_vols)
                prev_atm_iv = current_atm_iv
                prev_spot = spot_today

            iv_changes[ticker] = daily_changes

    results_df = pd.DataFrame(iv_changes, index=dates)
    return results_df.dropna(how='all')

# 9:30
import pandas as pd

# The path to the HDF5 file you already created
db_path = 'vol_surfaces.h5'

print(f"Inspecting structure of: {db_path}\n" + "="*40)

with pd.HDFStore(db_path, mode='r') as store:
    # Get all the keys (paths) in the file
    all_keys = store.keys()
    
    # Print the first 20 keys to see the structure
    print("First 20 keys in the file:")
    for key in all_keys[:20]:
        print(key)
        
    print(f"\nTotal items in file: {len(all_keys)}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Load All Data for One Stock into Memory ---

# Choose a single stock to process for this test
ticker_to_process = 'AAPL-UQ' 
ticker_key_name = ticker_to_process.replace("-", "_").replace(".", "")
maturity_in_years = 30 / 365.25

# This dictionary will hold all the data we load from the file
# Keys will be dates, values will be another dict: {'surface': df, 'spot': spot_price}
stock_data = {}

# Open the file ONCE to pull all data for our chosen stock
with pd.HDFStore(db_path, mode='r') as store:
    for key in store.keys():
        if f'/{ticker_key_name}/' in key:
            # Extract the date from the key string
            date_str = key.split('/')[-1].replace('d', '')
            date = pd.to_datetime(date_str, format='%Y%m%d')
            
            # Load the surface and its metadata into our dictionary
            surface_df = store[key]
            metadata = store.get_storer(key).attrs.metadata
            stock_data[date] = {'surface': surface_df, 'spot': metadata['spot']}

# Convert the dictionary into a DataFrame for easy processing
df = pd.DataFrame.from_dict(stock_data, orient='index').sort_index()


# --- Step 2: Calculate the IV Change Series using Pandas ---

# Create columns for the PREVIOUS day's data using .shift()
df['prev_spot'] = df['spot'].shift(1)
df['prev_surface'] = df['surface'].shift(1)

# This function calculates the change for a single row (one day)
def calculate_iv_change_for_row(row):
    # Skip the first row or any day missing previous data
    if pd.isna(row['prev_spot']) or row['prev_surface'] is None:
        return np.nan

    # --- Get previous day's ATM IV ---
    prev_surface = row['prev_surface']
    
    # Because of the 'm_x_y' format, we must parse them back to numbers
    iv_cols = [c for c in prev_surface.columns if c.startswith('m_')]
    moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
    
    # The string format requires sorting to ensure interpolation works correctly
    sorted_indices = np.argsort(moneyness_levels)
    moneyness_levels = np.array(moneyness_levels)[sorted_indices]
    prev_iv_surface_values = prev_surface[np.array(iv_cols)[sorted_indices]].values
    
    # Interpolate to get the 30-day constant maturity slice
    prev_expiries = prev_surface['Expiry'].values
    prev_constant_maturity_vols = np.apply_along_axis(
        lambda vol_col: np.interp(maturity_in_years, prev_expiries, vol_col), 0, prev_iv_surface_values
    )
    
    # Find the ATM IV from the previous day
    prev_atm_iv = np.interp(1.0, moneyness_levels, prev_constant_maturity_vols)

    # --- Get today's IV for yesterday's strike ---
    today_surface = row['surface']
    today_spot = row['spot']
    
    # Repeat the parsing for today's surface
    iv_cols = [c for c in today_surface.columns if c.startswith('m_')]
    moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
    sorted_indices = np.argsort(moneyness_levels)
    moneyness_levels = np.array(moneyness_levels)[sorted_indices]
    today_iv_surface_values = today_surface[np.array(iv_cols)[sorted_indices]].values

    today_expiries = today_surface['Expiry'].values
    today_constant_maturity_vols = np.apply_along_axis(
        lambda vol_col: np.interp(maturity_in_years, today_expiries, vol_col), 0, today_iv_surface_values
    )
    
    # Calculate moneyness of yesterday's strike on today's surface
    moneyness_of_prev_strike = row['prev_spot'] / today_spot
    
    # Interpolate to find the IV
    iv_of_prev_strike_today = np.interp(moneyness_of_prev_strike, moneyness_levels, today_constant_maturity_vols)
    
    # The final result for the day
    return iv_of_prev_strike_today - prev_atm_iv

# Apply the function to every row to calculate the series
df['iv_change'] = df.apply(calculate_iv_change_for_row, axis=1)


# --- Step 3: Visualize the Resulting Series ---

plt.style.use('seaborn-v0_8-grid')
fig, ax = plt.subplots(figsize=(15, 7))

df['iv_change'].plot(ax=ax, label='Daily IV Change', marker='.', linestyle='-')

ax.set_title(f'Calculated Fixed-Strike IV Change Series for {ticker_to_process}', fontsize=16)
ax.set_xlabel('Date', fontsize=12)
ax.set_ylabel('Absolute Change in Implied Volatility', fontsize=12)
ax.axhline(0, color='red', linestyle='--', lw=1)
ax.legend()
plt.tight_layout()
plt.show()

# Display the head and tail of the final data
print(f"\nFinal DataFrame with calculated changes for {ticker_to_process}:\n" + "="*60)
print(df[['spot', 'prev_spot', 'iv_change']].dropna().head())
print("...")
print(df[['spot', 'prev_spot', 'iv_change']].dropna().tail())

# 9:45

import pandas as pd
import numpy as np

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity_in_years=30/365.25):
    
    all_results = {}

    def calculate_iv_change_for_row(row):
        if pd.isna(row['prev_spot']) or row['prev_surface'] is None:
            return np.nan

        prev_surface = row['prev_surface']
        iv_cols = [c for c in prev_surface.columns if c.startswith('m_')]
        moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
        sorted_indices = np.argsort(moneyness_levels)
        moneyness_levels = np.array(moneyness_levels)[sorted_indices]
        prev_iv_surface_values = prev_surface[np.array(iv_cols)[sorted_indices]].values
        prev_expiries = prev_surface['Expiry'].values
        prev_constant_maturity_vols = np.apply_along_axis(
            lambda vol_col: np.interp(maturity_in_years, prev_expiries, vol_col), 0, prev_iv_surface_values
        )
        prev_atm_iv = np.interp(1.0, moneyness_levels, prev_constant_maturity_vols)

        today_surface = row['surface']
        today_spot = row['spot']
        iv_cols = [c for c in today_surface.columns if c.startswith('m_')]
        moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
        sorted_indices = np.argsort(moneyness_levels)
        moneyness_levels = np.array(moneyness_levels)[sorted_indices]
        today_iv_surface_values = today_surface[np.array(iv_cols)[sorted_indices]].values
        today_expiries = today_surface['Expiry'].values
        today_constant_maturity_vols = np.apply_along_axis(
            lambda vol_col: np.interp(maturity_in_years, today_expiries, vol_col), 0, today_iv_surface_values
        )

        moneyness_of_prev_strike = row['prev_spot'] / today_spot
        iv_of_prev_strike_today = np.interp(moneyness_of_prev_strike, moneyness_levels, today_constant_maturity_vols)

        return iv_of_prev_strike_today - prev_atm_iv

    with pd.HDFStore(db_path, mode='r') as store:
        all_file_keys = store.keys()
        
        for ticker in tickers:
            ticker_key_name = f"/{ticker.replace('-', '_').replace('.', '')}/"
            stock_data = {}

            keys_for_ticker = [key for key in all_file_keys if key.startswith(ticker_key_name)]

            for key in keys_for_ticker:
                date_str = key.split('/')[-1].replace('d', '')
                date = pd.to_datetime(date_str, format='%Y%m%d')
                
                surface_df = store.get(key)
                metadata = store.get_storer(key).attrs.metadata
                stock_data[date] = {'surface': surface_df, 'spot': metadata['spot']}
            
            if not stock_data:
                continue

            df = pd.DataFrame.from_dict(stock_data, orient='index').sort_index()
            df = df.loc[start_date:end_date]
            
            if df.empty:
                continue

            df['prev_spot'] = df['spot'].shift(1)
            df['prev_surface'] = df['surface'].shift(1)
            
            all_results[ticker] = df.apply(calculate_iv_change_for_row, axis=1)

    final_df = pd.DataFrame(all_results)
    return final_df

# 10:20

import pandas as pd

# --- Configuration ---
db_path = 'vol_surfaces.h5'
date_to_check = '2023-01-03' # Pick any date that exists in your dataset
ticker1 = 'AAPL-UQ'
ticker2 = 'MSFT-US' # Pick two different tickers from your top 50 list

# --- Debugging Code ---
print("--- HDF5 Data Integrity Check ---")

# Construct the precise keys for the HDF5 file
key1 = f"/{ticker1.replace('-', '_').replace('.', '')}/{pd.to_datetime(date_to_check).strftime('d%Y%m%d')}"
key2 = f"/{ticker2.replace('-', '_').replace('.', '')}/{pd.to_datetime(date_to_check).strftime('d%Y%m%d')}"

with pd.HDFStore(db_path, mode='r') as store:
    # Verify both keys actually exist in the file
    if key1 not in store or key2 not in store:
        print(f"Error: A key for date {date_to_check} is missing for one of the tickers.")
    else:
        # Load the data and metadata for both tickers
        df1 = store.get(key1)
        meta1 = store.get_storer(key1).attrs.metadata
        
        df2 = store.get(key2)
        meta2 = store.get_storer(key2).attrs.metadata

        # Print the spot price and head of each DataFrame
        print(f"\nData for {ticker1} on {date_to_check}:")
        print(f"Spot Price: {meta1['spot']}")
        print(df1.head(3))
        
        print(f"\nData for {ticker2} on {date_to_check}:")
        print(f"Spot Price: {meta2['spot']}")
        print(df2.head(3))

        # Programmatically check if the two DataFrames are identical
        are_they_equal = df1.equals(df2)
        
        print("\n--- Verification Result ---")
        if are_they_equal:
            print("ðŸ”´ CRITICAL FLAW CONFIRMED: The data saved for both tickers is identical.")
        else:
            print("ðŸŸ¢ The data for the two tickers is different, as expected.")


# 10:30

import pandas as pd
import numpy as np

def store_vol_surfaces_to_hdf(spy_weights_df, start_date, end_date, output_path='vol_surfaces.h5'):
    
    top_50_tickers = spy_weights_df.nlargest(50, 'ID().WEIGHTS')['CMDB Ticker'].tolist()
    all_tickers = list(set(top_50_tickers + ['SPY-UP', 'SPX']))
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))

    with pd.HDFStore(output_path, mode='w', complevel=9, complib='blosc') as store:
        for ticker in all_tickers:
            for date in dates:
                surface_df = get_iv_surface(ticker, date)

                if surface_df is not None:
                    rename_map = {c: f"m_{str(c).replace('.', '_')}" for c in surface_df.columns if isinstance(c, (int, float))}
                    
                    # --- THE FIX: Avoid inplace=True and reassign the DataFrame ---
                    surface_df = surface_df.rename(columns=rename_map)
                    
                    key = f'/{ticker.replace("-", "_").replace(".", "")}/{date.strftime("d%Y%m%d")}'
                    
                    store.put(key, surface_df, format='table')
                    
                    store.get_storer(key).attrs.metadata = {
                        'spot': surface_df.spot,
                        'valuation_date': surface_df.valuation_date
                    }

# 16:30

import matplotlib.pyplot as plt
import seaborn as sns

def plot_iv_series(iv_change_series, benchmarks):
    plt.style.use('seaborn-v0_8-grid')
    fig, ax = plt.subplots(figsize=(18, 9))

    for ticker in iv_change_series.columns:
        if ticker in benchmarks:
            ax.plot(iv_change_series.index, iv_change_series[ticker], lw=2.0, label=ticker)
        else:
            ax.plot(iv_change_series.index, iv_change_series[ticker], lw=0.7, linestyle='--')

    ax.set_title('Implied Volatility Change Series', fontsize=16)
    ax.set_xlabel('Date', fontsize=12)
    ax.set_ylabel('Absolute Change in IV', fontsize=12)
    ax.legend()
    plt.tight_layout()
    plt.show()

# Example Usage:
# benchmarks = ['SPY-UP', 'SPX'] 
# plot_iv_series(iv_change_series, benchmarks)


import pandas as pd
import numpy as np

# Example Usage:
# dispersion_betas = calculate_dispersion_matrix(iv_change_series, benchmarks)


from statsmodels.tsa.stattools import adfuller

def analyze_stationarity_and_drift(iv_change_series):
    results = []
    for ticker in iv_change_series.columns:
        series = iv_change_series[ticker].dropna()
        adf_result = adfuller(series)
        
        # Decision: 'Drift' is interpreted as the mean of the daily IV change series.
        drift = series.mean() * 252 # Annualized drift
        
        results.append({
            'Ticker': ticker,
            'ADF_p_value': adf_result[1],
            'Is_Stationary_at_5%': adf_result[1] < 0.05,
            'Annualized_Drift': drift
        })

    results_df = pd.DataFrame(results).set_index('Ticker')
    results_df = results_df.sort_values(by='Annualized_Drift', ascending=False)
    return results_df

# Example Usage:
# stationarity_report = analyze_stationarity_and_drift(iv_change_series)
# print(stationarity_report)


from arch import arch_model

def analyze_volatility_clustering(iv_change_series):
    garch_results = []
    for ticker in iv_change_series.columns:
        series = iv_change_series[ticker].dropna() * 100
        model = arch_model(series, vol='Garch', p=1, q=1, dist='ged')
        res = model.fit(disp='off')
        
        garch_results.append({
            'Ticker': ticker,
            'alpha[1]': res.params['alpha[1]'],
            'beta[1]': res.params['beta[1]'],
            'alpha+beta': res.params['alpha[1]'] + res.params['beta[1]']
        })

    garch_df = pd.DataFrame(garch_results).set_index('Ticker').sort_values('alpha+beta', ascending=False)
    print("--- GARCH Volatility Clustering Results ---")
    print(garch_df.head(10))

    corr_matrix = iv_change_series.corr()
    
    # Decision: Use seaborn's clustermap to automatically group similar stocks.
    sns.clustermap(corr_matrix, cmap='viridis', annot=False, figsize=(14, 14))
    plt.suptitle('Hierarchical Clustering of IV Change Correlation', y=1.02, fontsize=16)
    plt.show()
    
    return garch_df, corr_matrix

# Example Usage:
# garch_report, correlation_matrix = analyze_volatility_clustering(iv_change_series)


def vol_of_vol_predictive_model(iv_change_series, base_ticker, target_ticker):
    base_series = iv_change_series[base_ticker].dropna() * 100
    target_series = iv_change_series[target_ticker].dropna() * 100
    
    # Align the series
    df = pd.DataFrame({'base': base_series, 'target': target_series}).dropna()

    # Fit GARCH to both to get conditional vol series
    base_garch = arch_model(df['base'], vol='Garch', p=1, q=1).fit(disp='off')
    target_garch = arch_model(df['target'], vol='Garch', p=1, q=1).fit(disp='off')
    
    vols_df = pd.DataFrame({
        'base_vol': base_garch.conditional_volatility,
        'target_vol': target_garch.conditional_volatility
    })
    
    # Predictive regression: Target Vol(t) = C + Beta * Base Vol(t)
    y = vols_df['target_vol']
    X = sm.add_constant(vols_df['base_vol']) # Using statsmodels for a clean summary
    
    model = sm.OLS(y, X).fit()
    print(f"\n--- Predictive Model: {target_ticker} Vol vs. {base_ticker} Vol ---")
    print(model.summary())
    
    # Visualize the relationship
    sns.regplot(x='base_vol', y='target_vol', data=vols_df, line_kws={'color': 'red'})
    plt.title(f'Conditional Volatility Relationship: {base_ticker} vs. {target_ticker}', fontsize=16)
    plt.xlabel(f'{base_ticker} Conditional Volatility')
    plt.ylabel(f'{target_ticker} Conditional Volatility')
    plt.show()

# Example Usage:
# import statsmodels.api as sm
# base_ticker = 'SPY-UP'
# target_ticker = 'AAPL-UQ' # Choose any stock from your list
# vol_of_vol_predictive_model(iv_change_series, base_ticker, target_ticker)


# 10:35


import pandas as pd
import numpy as np

def winsorize_and_report_volatility(iv_change_series, method='sigma', threshold=3):
    
    original_vol = iv_change_series.std().sort_values(ascending=False)
    print("--- Top 10 Most Volatile Series (Pre-Winsorization) ---")
    print(original_vol.head(10))

    winsorized_df = iv_change_series.copy()

    for col in winsorized_df.columns:
        series = winsorized_df[col].dropna()

        if method == 'sigma':
            mean = series.mean()
            std_dev = series.std()
            upper_bound = mean + threshold * std_dev
            lower_bound = mean - threshold * std_dev
        
        elif method == 'mad':
            median = series.median()
            # Scaled MAD for comparison with standard deviation
            mad = (series - median).abs().median() * 1.4826 
            upper_bound = median + threshold * mad
            lower_bound = median - threshold * mad
        
        else:
            return None

        winsorized_df[col] = winsorized_df[col].clip(lower=lower_bound, upper=upper_bound)

    return winsorized_df

# Example Usage:
# winsorized_iv_changes = winsorize_and_report_volatility(iv_change_series, method='mad', threshold=3.5)


import matplotlib.pyplot as plt
import seaborn as sns

def analyze_correlation_stability(iv_change_series, ticker1, ticker2, window=60):
    
    static_corr = iv_change_series.corr()
    
    print(f"--- Static Correlation for {ticker1} and {ticker2}: {static_corr.loc[ticker1, ticker2]:.4f} ---")
    
    rolling_corr = iv_change_series[ticker1].rolling(window=window).corr(iv_change_series[ticker2])
    
    fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True, gridspec_kw={'height_ratios': [2, 1]})
    
    axes[0].plot(iv_change_series.index, iv_change_series[ticker1], label=ticker1, lw=1.0)
    axes[0].plot(iv_change_series.index, iv_change_series[ticker2], label=ticker2, lw=1.0)
    axes[0].set_title(f'IV Change Series: {ticker1} vs. {ticker2}', fontsize=14)
    axes[0].set_ylabel('IV Change')
    axes[0].legend()
    
    axes[1].plot(rolling_corr.index, rolling_corr, label=f'{window}-Day Rolling Correlation', color='red')
    axes[1].axhline(static_corr.loc[ticker1, ticker2], color='black', linestyle='--', lw=1.5, label='Static Correlation')
    axes[1].set_title('Correlation Stability Analysis', fontsize=14)
    axes[1].set_ylabel('Correlation')
    axes[1].set_xlabel('Date')
    axes[1].legend()
    
    plt.tight_layout()
    plt.show()

    return static_corr, rolling_corr

# Example Usage:
# Choose two tickers to compare, for example a tech giant and a benchmark.
# static_corr_matrix, rolling_corr_series = analyze_correlation_stability(iv_change_series, 'AAPL-UQ', 'SPY-UP', window=90)



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_all_pairs_correlation(iv_change_series, benchmarks, window=60):
    
    non_benchmark_tickers = [t for t in iv_change_series.columns if t not in benchmarks]
    df = iv_change_series[non_benchmark_tickers]
    
    # --- Static Correlation Matrix and Heatmap ---
    static_corr_matrix = df.corr()
    
    plt.figure(figsize=(16, 14))
    sns.heatmap(static_corr_matrix, cmap='viridis', annot=False)
    plt.title('Static Pairwise Correlation Matrix', fontsize=16)
    plt.show()

    # --- Rolling Correlation Stability Analysis ---
    results = []
    
    for i in range(len(df.columns)):
        for j in range(i + 1, len(df.columns)):
            ticker1 = df.columns[i]
            ticker2 = df.columns[j]
            
            rolling_corr = df[ticker1].rolling(window=window).corr(df[ticker2]).dropna()
            
            if not rolling_corr.empty:
                results.append({
                    'pair': f"{ticker1}-{ticker2}",
                    'mean_corr': rolling_corr.mean(),
                    'std_dev_corr': rolling_corr.std()
                })

    corr_stability_df = pd.DataFrame(results)
    
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(12, 8))
    
    sns.scatterplot(
        data=corr_stability_df,
        x='mean_corr',
        y='std_dev_corr',
        hue='mean_corr',
        size='std_dev_corr',
        sizes=(20, 200),
        palette='viridis',
        legend=False
    )
    
    plt.title('Pairwise Correlation Stability Map', fontsize=16)
    plt.xlabel('Mean Rolling Correlation', fontsize=12)
    plt.ylabel('Standard Deviation of Rolling Correlation (Volatility)', fontsize=12)

    most_stable_pair = corr_stability_df.sort_values(by=['mean_corr', 'std_dev_corr'], ascending=[False, True]).iloc[0]
    plt.text(most_stable_pair['mean_corr'], most_stable_pair['std_dev_corr'], f"  Most Stable:\n  {most_stable_pair['pair']}", ha='left')

    plt.tight_layout()
    plt.show()

    return static_corr_matrix, corr_stability_df

# Example Usage:
# benchmarks = ['SPY-UP', 'SPX']
# static_matrix, stability_report = analyze_all_pairs_correlation(iv_change_series, benchmarks, window=90)
# print("\n--- Top 10 Most Stable & Correlated Pairs ---")
# print(stability_report.sort_values(by='mean_corr', ascending=False).head(10))


# 2:25

import numpy as np
from numpy.linalg import inv, pinv

def hedge_weights(z, w, A, Sigma, x, use_pinv=False):
    """
    Returns optimal hedge weights u* that minimise E[(Î”P)^2].
    
    Parameters
    ----------
    z      : (n,)    dollar vega 1% per single name
    w      : (n,)    portfolio weights
    A      : (n,m)   dispersion matrix
    Sigma  : (n,n)   vol-move correlation / covariance
    x      : (m,)    dollar vega 1% per index instrument
    use_pinv : bool  set True to fall back on Moore-Penrose
    
    Returns
    -------
    u_star : (m,)    contracts (or vega notional / x) for each hedge
    h_star : (m,)    dollar vega per 1% change in each index
    """
    q = z * w                            # element-wise
    lhs = A.T @ Sigma @ A
    rhs = A.T @ Sigma @ q
    solver = pinv if use_pinv else inv
    h_star = solver(lhs) @ rhs
    u_star = h_star / x                  # element-wise
    return u_star, h_star

n = 10
m = 3
z      = np.full(n, 1e5)         # $100k each
w      = np.full(n, 1/n)         # equal weights
A      = np.random.uniform(0.3, 0.9, size=(n, m))   # toy dispersion
Sigma  = np.eye(n)               # uncorrelated vols for demo
x      = np.full(m, 1e6)         # say $1 M vega per index option

u_star, h_star = hedge_weights(z, w, A, Sigma, x)
print("Index-contract hedge weights  u*:", u_star)
print("Dollar-vega exposures         h*:", h_star)

# 2:40

import pandas as pd
import numpy as np

def calculate_gls_optimal_hedge(
    portfolio_weights,
    vega_1_percent_stocks,
    vega_1_percent_hedges,
    dispersion_matrix,
    iv_change_series
):
    
    stock_tickers = portfolio_weights.index
    hedge_tickers = vega_1_percent_hedges.index

    q_dollar_vega = portfolio_weights * vega_1_percent_stocks
    
    # Calculate the covariance matrix Sigma from the time series
    Sigma = iv_change_series[stock_tickers].cov()
    
    # Align matrices to the portfolio's stocks and hedges
    A = dispersion_matrix.loc[stock_tickers, hedge_tickers].values
    q = q_dollar_vega.values
    
    # Implement the GLS formula: h = (A_T * Sigma * A)^-1 * (A_T * Sigma * q)
    A_T_Sigma = A.T @ Sigma
    A_T_Sigma_A = A_T_Sigma @ A
    
    # Use the Moore-Penrose pseudo-inverse for numerical stability
    A_T_Sigma_A_inv = np.linalg.pinv(A_T_Sigma_A)
    
    h_star = A_T_Sigma_A_inv @ A_T_Sigma @ q
    
    # Convert dollar-vega amounts (h) back to contract weights (u)
    u_star = h_star / vega_1_percent_hedges.values

    results = pd.DataFrame({
        'Hedge_Dollar_Vega_h': h_star,
        'Hedge_Contract_Weights_u': u_star
    }, index=hedge_tickers)
    
    return results

# --- Test Case ---

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# --- Setup from existing data ---
# Assume 'iv_change_series' is your DataFrame of IV changes.
# Assume 'benchmarks' is your list of hedge instrument tickers.

stock_tickers = [t for t in iv_change_series.columns if t not in benchmarks]
hedge_tickers = benchmarks

# --- Define Portfolio and Vega Assumptions ---
portfolio_weights = pd.Series(1/len(stock_tickers), index=stock_tickers)
vega_1_percent_stocks = pd.Series(100000.0, index=stock_tickers)

# Decision: Assume a Vega 1% for the hedging instruments.
vega_1_percent_hedges = pd.Series({
    'SPY-UP': 25000.0, 
    'SPX': 35000.0
}, index=hedge_tickers)


# --- Calculate Dispersion Matrix 'A' from Time Series ---
# Based on the model Î”Ï„ = Aáµ€Î”Ïƒ, we regress each benchmark's IV change 
# against all of the stock IV changes to find the coefficients that form Aáµ€.

df_clean = iv_change_series.dropna()
X_train = df_clean[stock_tickers]
A_T_list = []

for ticker in hedge_tickers:
    y_train = df_clean[ticker]
    model = LinearRegression(fit_intercept=False).fit(X_train, y_train)
    A_T_list.append(model.coef_)

A_T = np.array(A_T_list)
A = pd.DataFrame(A_T.T, index=stock_tickers, columns=hedge_tickers)


# --- Calculate the Optimal Hedge ---
optimal_hedge = calculate_gls_optimal_hedge(
    portfolio_weights=portfolio_weights,
    vega_1_percent_stocks=vega_1_percent_stocks,
    vega_1_percent_hedges=vega_1_percent_hedges,
    dispersion_matrix=A,
    iv_change_series=iv_change_series
)


# --- Print Final Result ---
print("--- GLS Optimal Hedge Weights ---\n")
print(optimal_hedge.round(4))

# 3:00

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

def calculate_dispersion_matrix(iv_change_series, benchmarks):
    
    stock_tickers = [t for t in iv_change_series.columns if t not in benchmarks]
    df_clean = iv_change_series.dropna()
    
    X_train = df_clean[stock_tickers]
    A_T_list = []

    for ticker in benchmarks:
        y_train = df_clean[ticker]
        model = LinearRegression(fit_intercept=False).fit(X_train, y_train)
        A_T_list.append(model.coef_)

    A_T = np.array(A_T_list)
    dispersion_A = pd.DataFrame(A_T.T, index=stock_tickers, columns=benchmarks)
    
    return dispersion_A

import matplotlib.pyplot as plt
import seaborn as sns

def plot_dispersion_matrix(dispersion_matrix):
    
    plt.figure(figsize=(10, 12))
    sns.heatmap(dispersion_matrix, cmap='coolwarm', annot=False)
    plt.title('Dispersion Matrix A', fontsize=16)
    plt.xlabel('Hedge Instruments (Benchmarks)')
    plt.ylabel('Portfolio Stocks')
    plt.show()

# --- Example Usage ---
#
# A_matrix = calculate_dispersion_matrix(iv_change_series, benchmarks)
#
# plot_dispersion_matrix(A_matrix)

