#import pandas as pd
import numpy as np

def get_constant_maturity_iv(tickers, start_date, end_date, maturity=30/365.25):
    """
    Generates a time series of constant maturity ATM implied volatility for multiple tickers.

    Args:
        tickers (list): List of stock tickers.
        start_date (str): Start date in 'YYYY-MM-DD' format.
        end_date (str): End date in 'YYYY-MM-DD' format.
        maturity (float): Desired constant maturity in years (e.g., 30 days = 30/365.25).

    Returns:
        pd.DataFrame: A DataFrame with dates as the index and IV time series as columns.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    all_ivs = {}

    for ticker in tickers:
        ticker_ivs = []
        for date in dates:
            try:
                surface = get_iv_surface(ticker, date) # User's provided function
                expiries = surface['Expiry'].values
                atm_vols = surface['1.0'].values
                
                # Linear interpolation to find the constant maturity IV
                constant_iv = np.interp(maturity, expiries, atm_vols)
                ticker_ivs.append(constant_iv)
            except Exception as e:
                ticker_ivs.append(np.nan) # Handle cases where data is not available
        all_ivs[ticker] = ticker_ivs
        
    df_iv = pd.DataFrame(all_ivs, index=dates)
    df_iv.dropna(how='all', inplace=True)
    return df_iv

# Example Usage:
# spy_components = spy_weights['CMDB Ticker'].tolist()
# iv_timeseries = get_constant_maturity_iv(spy_components, '2022-01-01', '2023-12-31')


def analyze_iv_comovement(df_iv, benchmark_ticker, stress_threshold=0.02):
    """
    Analyzes the co-movement of implied volatilities against a benchmark.

    Args:
        df_iv (pd.DataFrame): Time series of implied volatilities.
        benchmark_ticker (str): The ticker to use as the benchmark (e.g., 'SPY-US').
        stress_threshold (float): The daily percentage drop in benchmark to define a stress day.

    Returns:
        dict: A dictionary containing correlation and beta matrices for all, stress, and regular periods.
    """
    iv_returns = df_iv.pct_change().dropna()
    benchmark_returns = iv_returns[benchmark_ticker]
    
    # Identify stress periods
    stress_days = benchmark_returns < -stress_threshold
    regular_days = ~stress_days

    # Calculate correlation
    corr_all = iv_returns.corr()
    corr_stress = iv_returns[stress_days].corr()
    corr_regular = iv_returns[regular_days].corr()

    # Calculate scaling effect (beta)
    betas = iv_returns.apply(lambda x: np.polyfit(benchmark_returns, x, 1)[0])
    
    return {
        "correlation_all": corr_all,
        "correlation_stress": corr_stress,
        "correlation_regular": corr_regular,
        "scaling_betas": betas
    }

# Example Usage:
# comovement_results = analyze_iv_comovement(iv_timeseries, 'SPY-US')
# print("Scaling Betas:\n", comovement_results['scaling_betas'])


from arch import arch_model

def calculate_vol_of_vol(df_iv):
    """
    Calculates the volatility of implied volatility using three different methods.

    Args:
        df_iv (pd.DataFrame): Time series of implied volatilities.

    Returns:
        dict: A dictionary of DataFrames for each vol-of-vol calculation method.
    """
    iv_returns = df_iv.pct_change().dropna()
    
    # 1. 21-Day Rolling Window
    vol_rolling = iv_returns.rolling(window=21).std() * np.sqrt(252)

    # 2. EWMA
    vol_ewma = iv_returns.ewm(span=21, adjust=False).std() * np.sqrt(252)
    
    # 3. EGARCH(1,1)
    def fit_egarch(series):
        model = arch_model(series * 100, p=1, o=1, q=1, vol='EGARCH')
        res = model.fit(disp='off')
        return res.conditional_volatility / 100
        
    vol_egarch = iv_returns.apply(fit_egarch, axis=0) * np.sqrt(252)

    return {
        "rolling_21d": vol_rolling,
        "ewma_21d": vol_ewma,
        "egarch_1_1": vol_egarch
    }

# Example Usage:
# vol_of_vol_results = calculate_vol_of_vol(iv_timeseries)
# print("EWMA Vol of Vol:\n", vol_of_vol_results['ewma_21d'].tail())



from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

def verify_power_law_decay(ticker, valuation_date):
    """
    Fits a power law to the IV term structure using L2 regularized regression.
    The model is IV = c + b1*T^(-0.5) + b2*T^(-1.0), fit via log-log regression.

    Args:
        ticker (str): The stock ticker.
        valuation_date (str): The valuation date 'YYYY-MM-DD'.

    Returns:
        dict: A dictionary with the regression model and its parameters.
    """
    surface = get_iv_surface(ticker, pd.to_datetime(valuation_date))
    surface = surface.loc[surface['Expiry'] > 0]
    
    X = np.log(surface[['Expiry']].values)
    y = np.log(surface['1.0'].values)
    
    # Using Ridge (L2 regularization) on a log-log plot to find the power
    model = make_pipeline(PolynomialFeatures(1), Ridge(alpha=1.0))
    model.fit(X, y)
    
    # The coefficient of the log(T) term is the power 'alpha'
    # log(IV) = log(k) + alpha * log(T)
    alpha = model.named_steps['ridge'].coef_[1]
    
    return {
        "model": model,
        "power_law_exponent (alpha)": alpha
    }

# Example Usage:
# power_law_fit = verify_power_law_decay('AAPL-UQ', '2023-06-30')
# print(f"Fitted Power Law Exponent: {power_law_fit['power_law_exponent (alpha)']:.4f}")



def backtest_beta_hedging(portfolio_tickers, hedge_instrument, df_iv, betas):
    """
    Runs a simple backtest for a beta-weighted Vega hedge.

    Args:
        portfolio_tickers (list): List of tickers in the long portfolio.
        hedge_instrument (str): The ticker for the hedging instrument (e.g., 'SPY-US').
        df_iv (pd.DataFrame): Time series of implied volatilities.
        betas (pd.Series): Series of scaling betas against the hedge instrument.

    Returns:
        pd.DataFrame: A DataFrame showing the unhedged vs. hedged portfolio IV changes.
    """
    iv_returns = df_iv.pct_change().dropna()

    # Assume an equally-weighted portfolio (long 1 unit of IV in each stock)
    portfolio_iv_returns = iv_returns[portfolio_tickers].mean(axis=1)

    # Calculate the portfolio's aggregate beta
    portfolio_beta = betas[portfolio_tickers].mean()
    
    # The optimal hedge ratio is the portfolio's beta
    # We short 'portfolio_beta' units of the hedge instrument's IV
    hedge_iv_returns = portfolio_beta * iv_returns[hedge_instrument]
    
    # Calculate hedged portfolio returns
    hedged_portfolio_iv_returns = portfolio_iv_returns - hedge_iv_returns
    
    # Create results DataFrame
    results = pd.DataFrame({
        'unhedged_iv_change': portfolio_iv_returns,
        'hedged_iv_change': hedged_portfolio_iv_returns
    })
    
    print(f"Unhedged Volatility: {results['unhedged_iv_change'].std() * 100:.4f}%")
    print(f"Hedged Volatility:   {results['hedged_iv_change'].std() * 100:.4f}%")
    
    return results

# Example Usage:
# Assume we ran Module 2 and got the betas
# betas = comovement_results['scaling_betas']
# my_portfolio = ['AAPL-UQ', 'MSFT-US', 'NVDA-US'] # Our proxy portfolio
# hedge_results = backtest_beta_hedging(my_portfolio, 'SPY-US', iv_timeseries, betas)


import pandas as pd
import numpy as np

def store_vol_surfaces_to_hdf(spy_weights_df, start_date, end_date, output_path='vol_surfaces.h5'):
    """
    Fetches and stores IV surfaces in an HDF5 file with string-formatted columns.
    """
    top_50_tickers = spy_weights_df.nlargest(50, 'ID().WEIGHTS')['CMDB Ticker'].tolist()
    all_tickers = list(set(top_50_tickers + ['SPY-UP', 'SPX']))
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))

    with pd.HDFStore(output_path, mode='w', complevel=9, complib='blosc') as store:
        for ticker in all_tickers:
            for date in dates:
                # This is your provided function to get the raw surface
                surface_df = get_iv_surface(ticker, date)

                # Convert numeric columns to valid string names (e.g., 1.0 -> 'm_1_0')
                rename_map = {c: f"m_{str(c).replace('.', '_')}" for c in surface_df.columns if isinstance(c, (int, float))}
                surface_df.rename(columns=rename_map, inplace=True)
                
                # Create a safe key for HDF5 storage
                key = f'/{ticker.replace("-", "_").replace(".", "")}/{date.strftime("d%Y%m%d")}'
                
                # Store the DataFrame
                store.put(key, surface_df, format='table')
                
                # Store metadata
                store.get_storer(key).attrs.metadata = {
                    'spot': surface_df.spot,
                    'valuation_date': surface_df.valuation_date
                }

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity=30/365.25):
    """
    Calculates fixed-strike IV changes using the corrected HDF5 file.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    iv_changes = {}

    with pd.HDFStore(db_path, mode='r') as store:
        for ticker in tickers:
            ticker_key_name = ticker.replace("-", "_").replace(".", "")
            prev_atm_iv = np.nan
            prev_spot = np.nan
            daily_changes = []

            for date in dates:
                key = f'/{ticker_key_name}/{date.strftime("d%Y%m%d")}'
                if key not in store.keys():
                    daily_changes.append(np.nan)
                    continue

                surface = store[key]
                metadata = store.get_storer(key).attrs.metadata
                spot_today = metadata['spot']
                
                iv_cols = [c for c in surface.columns if c.startswith('m_')]
                moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
                
                # Sort them to ensure they are in ascending order for interpolation
                sorted_indices = np.argsort(moneyness_levels)
                moneyness_levels = np.array(moneyness_levels)[sorted_indices]
                iv_surface_values = surface[np.array(iv_cols)[sorted_indices]].values
                
                expiries = surface['Expiry'].values
                constant_maturity_vols = np.apply_along_axis(
                    lambda vol_col: np.interp(maturity, expiries, vol_col), 0, iv_surface_values
                )

                if not np.isnan(prev_spot):
                    moneyness_of_prev_strike = prev_spot / spot_today
                    iv_of_prev_strike_today = np.interp(
                        moneyness_of_prev_strike, moneyness_levels, constant_maturity_vols
                    )
                    change = iv_of_prev_strike_today - prev_atm_iv
                    daily_changes.append(change)
                else:
                    daily_changes.append(np.nan)

                current_atm_iv = np.interp(1.0, moneyness_levels, constant_maturity_vols)
                prev_atm_iv = current_atm_iv
                prev_spot = spot_today

            iv_changes[ticker] = daily_changes

    results_df = pd.DataFrame(iv_changes, index=dates)
    return results_df.dropna(how='all')

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity=30/365.25):
    """
    Calculates fixed-strike IV changes using the corrected HDF5 file.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    iv_changes = {}

    with pd.HDFStore(db_path, mode='r') as store:
        for ticker in tickers:
            ticker_key_name = ticker.replace("-", "_").replace(".", "")
            prev_atm_iv = np.nan
            prev_spot = np.nan
            daily_changes = []

            for date in dates:
                key = f'/{ticker_key_name}/{date.strftime("d%Y%m%d")}'
                if key not in store.keys():
                    daily_changes.append(np.nan)
                    continue

                surface = store[key]
                metadata = store.get_storer(key).attrs.metadata
                spot_today = metadata['spot']
                
                iv_cols = [c for c in surface.columns if c.startswith('m_')]
                moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
                
                # Sort them to ensure they are in ascending order for interpolation
                sorted_indices = np.argsort(moneyness_levels)
                moneyness_levels = np.array(moneyness_levels)[sorted_indices]
                iv_surface_values = surface[np.array(iv_cols)[sorted_indices]].values
                
                expiries = surface['Expiry'].values
                constant_maturity_vols = np.apply_along_axis(
                    lambda vol_col: np.interp(maturity, expiries, vol_col), 0, iv_surface_values
                )

                if not np.isnan(prev_spot):
                    moneyness_of_prev_strike = prev_spot / spot_today
                    iv_of_prev_strike_today = np.interp(
                        moneyness_of_prev_strike, moneyness_levels, constant_maturity_vols
                    )
                    change = iv_of_prev_strike_today - prev_atm_iv
                    daily_changes.append(change)
                else:
                    daily_changes.append(np.nan)

                current_atm_iv = np.interp(1.0, moneyness_levels, constant_maturity_vols)
                prev_atm_iv = current_atm_iv
                prev_spot = spot_today

            iv_changes[ticker] = daily_changes

    results_df = pd.DataFrame(iv_changes, index=dates)
    return results_df.dropna(how='all')

# 9:30
import pandas as pd

# The path to the HDF5 file you already created
db_path = 'vol_surfaces.h5'

print(f"Inspecting structure of: {db_path}\n" + "="*40)

with pd.HDFStore(db_path, mode='r') as store:
    # Get all the keys (paths) in the file
    all_keys = store.keys()
    
    # Print the first 20 keys to see the structure
    print("First 20 keys in the file:")
    for key in all_keys[:20]:
        print(key)
        
    print(f"\nTotal items in file: {len(all_keys)}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Load All Data for One Stock into Memory ---

# Choose a single stock to process for this test
ticker_to_process = 'AAPL-UQ' 
ticker_key_name = ticker_to_process.replace("-", "_").replace(".", "")
maturity_in_years = 30 / 365.25

# This dictionary will hold all the data we load from the file
# Keys will be dates, values will be another dict: {'surface': df, 'spot': spot_price}
stock_data = {}

# Open the file ONCE to pull all data for our chosen stock
with pd.HDFStore(db_path, mode='r') as store:
    for key in store.keys():
        if f'/{ticker_key_name}/' in key:
            # Extract the date from the key string
            date_str = key.split('/')[-1].replace('d', '')
            date = pd.to_datetime(date_str, format='%Y%m%d')
            
            # Load the surface and its metadata into our dictionary
            surface_df = store[key]
            metadata = store.get_storer(key).attrs.metadata
            stock_data[date] = {'surface': surface_df, 'spot': metadata['spot']}

# Convert the dictionary into a DataFrame for easy processing
df = pd.DataFrame.from_dict(stock_data, orient='index').sort_index()


# --- Step 2: Calculate the IV Change Series using Pandas ---

# Create columns for the PREVIOUS day's data using .shift()
df['prev_spot'] = df['spot'].shift(1)
df['prev_surface'] = df['surface'].shift(1)

# This function calculates the change for a single row (one day)
def calculate_iv_change_for_row(row):
    # Skip the first row or any day missing previous data
    if pd.isna(row['prev_spot']) or row['prev_surface'] is None:
        return np.nan

    # --- Get previous day's ATM IV ---
    prev_surface = row['prev_surface']
    
    # Because of the 'm_x_y' format, we must parse them back to numbers
    iv_cols = [c for c in prev_surface.columns if c.startswith('m_')]
    moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
    
    # The string format requires sorting to ensure interpolation works correctly
    sorted_indices = np.argsort(moneyness_levels)
    moneyness_levels = np.array(moneyness_levels)[sorted_indices]
    prev_iv_surface_values = prev_surface[np.array(iv_cols)[sorted_indices]].values
    
    # Interpolate to get the 30-day constant maturity slice
    prev_expiries = prev_surface['Expiry'].values
    prev_constant_maturity_vols = np.apply_along_axis(
        lambda vol_col: np.interp(maturity_in_years, prev_expiries, vol_col), 0, prev_iv_surface_values
    )
    
    # Find the ATM IV from the previous day
    prev_atm_iv = np.interp(1.0, moneyness_levels, prev_constant_maturity_vols)

    # --- Get today's IV for yesterday's strike ---
    today_surface = row['surface']
    today_spot = row['spot']
    
    # Repeat the parsing for today's surface
    iv_cols = [c for c in today_surface.columns if c.startswith('m_')]
    moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
    sorted_indices = np.argsort(moneyness_levels)
    moneyness_levels = np.array(moneyness_levels)[sorted_indices]
    today_iv_surface_values = today_surface[np.array(iv_cols)[sorted_indices]].values

    today_expiries = today_surface['Expiry'].values
    today_constant_maturity_vols = np.apply_along_axis(
        lambda vol_col: np.interp(maturity_in_years, today_expiries, vol_col), 0, today_iv_surface_values
    )
    
    # Calculate moneyness of yesterday's strike on today's surface
    moneyness_of_prev_strike = row['prev_spot'] / today_spot
    
    # Interpolate to find the IV
    iv_of_prev_strike_today = np.interp(moneyness_of_prev_strike, moneyness_levels, today_constant_maturity_vols)
    
    # The final result for the day
    return iv_of_prev_strike_today - prev_atm_iv

# Apply the function to every row to calculate the series
df['iv_change'] = df.apply(calculate_iv_change_for_row, axis=1)


# --- Step 3: Visualize the Resulting Series ---

plt.style.use('seaborn-v0_8-grid')
fig, ax = plt.subplots(figsize=(15, 7))

df['iv_change'].plot(ax=ax, label='Daily IV Change', marker='.', linestyle='-')

ax.set_title(f'Calculated Fixed-Strike IV Change Series for {ticker_to_process}', fontsize=16)
ax.set_xlabel('Date', fontsize=12)
ax.set_ylabel('Absolute Change in Implied Volatility', fontsize=12)
ax.axhline(0, color='red', linestyle='--', lw=1)
ax.legend()
plt.tight_layout()
plt.show()

# Display the head and tail of the final data
print(f"\nFinal DataFrame with calculated changes for {ticker_to_process}:\n" + "="*60)
print(df[['spot', 'prev_spot', 'iv_change']].dropna().head())
print("...")
print(df[['spot', 'prev_spot', 'iv_change']].dropna().tail())

# 9:45

import pandas as pd
import numpy as np

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity_in_years=30/365.25):
    
    all_results = {}

    def calculate_iv_change_for_row(row):
        if pd.isna(row['prev_spot']) or row['prev_surface'] is None:
            return np.nan

        prev_surface = row['prev_surface']
        iv_cols = [c for c in prev_surface.columns if c.startswith('m_')]
        moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
        sorted_indices = np.argsort(moneyness_levels)
        moneyness_levels = np.array(moneyness_levels)[sorted_indices]
        prev_iv_surface_values = prev_surface[np.array(iv_cols)[sorted_indices]].values
        prev_expiries = prev_surface['Expiry'].values
        prev_constant_maturity_vols = np.apply_along_axis(
            lambda vol_col: np.interp(maturity_in_years, prev_expiries, vol_col), 0, prev_iv_surface_values
        )
        prev_atm_iv = np.interp(1.0, moneyness_levels, prev_constant_maturity_vols)

        today_surface = row['surface']
        today_spot = row['spot']
        iv_cols = [c for c in today_surface.columns if c.startswith('m_')]
        moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
        sorted_indices = np.argsort(moneyness_levels)
        moneyness_levels = np.array(moneyness_levels)[sorted_indices]
        today_iv_surface_values = today_surface[np.array(iv_cols)[sorted_indices]].values
        today_expiries = today_surface['Expiry'].values
        today_constant_maturity_vols = np.apply_along_axis(
            lambda vol_col: np.interp(maturity_in_years, today_expiries, vol_col), 0, today_iv_surface_values
        )

        moneyness_of_prev_strike = row['prev_spot'] / today_spot
        iv_of_prev_strike_today = np.interp(moneyness_of_prev_strike, moneyness_levels, today_constant_maturity_vols)

        return iv_of_prev_strike_today - prev_atm_iv

    with pd.HDFStore(db_path, mode='r') as store:
        all_file_keys = store.keys()
        
        for ticker in tickers:
            ticker_key_name = f"/{ticker.replace('-', '_').replace('.', '')}/"
            stock_data = {}

            keys_for_ticker = [key for key in all_file_keys if key.startswith(ticker_key_name)]

            for key in keys_for_ticker:
                date_str = key.split('/')[-1].replace('d', '')
                date = pd.to_datetime(date_str, format='%Y%m%d')
                
                surface_df = store.get(key)
                metadata = store.get_storer(key).attrs.metadata
                stock_data[date] = {'surface': surface_df, 'spot': metadata['spot']}
            
            if not stock_data:
                continue

            df = pd.DataFrame.from_dict(stock_data, orient='index').sort_index()
            df = df.loc[start_date:end_date]
            
            if df.empty:
                continue

            df['prev_spot'] = df['spot'].shift(1)
            df['prev_surface'] = df['surface'].shift(1)
            
            all_results[ticker] = df.apply(calculate_iv_change_for_row, axis=1)

    final_df = pd.DataFrame(all_results)
    return final_df

# 10:20

import pandas as pd

# --- Configuration ---
db_path = 'vol_surfaces.h5'
date_to_check = '2023-01-03' # Pick any date that exists in your dataset
ticker1 = 'AAPL-UQ'
ticker2 = 'MSFT-US' # Pick two different tickers from your top 50 list

# --- Debugging Code ---
print("--- HDF5 Data Integrity Check ---")

# Construct the precise keys for the HDF5 file
key1 = f"/{ticker1.replace('-', '_').replace('.', '')}/{pd.to_datetime(date_to_check).strftime('d%Y%m%d')}"
key2 = f"/{ticker2.replace('-', '_').replace('.', '')}/{pd.to_datetime(date_to_check).strftime('d%Y%m%d')}"

with pd.HDFStore(db_path, mode='r') as store:
    # Verify both keys actually exist in the file
    if key1 not in store or key2 not in store:
        print(f"Error: A key for date {date_to_check} is missing for one of the tickers.")
    else:
        # Load the data and metadata for both tickers
        df1 = store.get(key1)
        meta1 = store.get_storer(key1).attrs.metadata
        
        df2 = store.get(key2)
        meta2 = store.get_storer(key2).attrs.metadata

        # Print the spot price and head of each DataFrame
        print(f"\nData for {ticker1} on {date_to_check}:")
        print(f"Spot Price: {meta1['spot']}")
        print(df1.head(3))
        
        print(f"\nData for {ticker2} on {date_to_check}:")
        print(f"Spot Price: {meta2['spot']}")
        print(df2.head(3))

        # Programmatically check if the two DataFrames are identical
        are_they_equal = df1.equals(df2)
        
        print("\n--- Verification Result ---")
        if are_they_equal:
            print("🔴 CRITICAL FLAW CONFIRMED: The data saved for both tickers is identical.")
        else:
            print("🟢 The data for the two tickers is different, as expected.")


# 10:30

import pandas as pd
import numpy as np

def store_vol_surfaces_to_hdf(spy_weights_df, start_date, end_date, output_path='vol_surfaces.h5'):
    
    top_50_tickers = spy_weights_df.nlargest(50, 'ID().WEIGHTS')['CMDB Ticker'].tolist()
    all_tickers = list(set(top_50_tickers + ['SPY-UP', 'SPX']))
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))

    with pd.HDFStore(output_path, mode='w', complevel=9, complib='blosc') as store:
        for ticker in all_tickers:
            for date in dates:
                surface_df = get_iv_surface(ticker, date)

                if surface_df is not None:
                    rename_map = {c: f"m_{str(c).replace('.', '_')}" for c in surface_df.columns if isinstance(c, (int, float))}
                    
                    # --- THE FIX: Avoid inplace=True and reassign the DataFrame ---
                    surface_df = surface_df.rename(columns=rename_map)
                    
                    key = f'/{ticker.replace("-", "_").replace(".", "")}/{date.strftime("d%Y%m%d")}'
                    
                    store.put(key, surface_df, format='table')
                    
                    store.get_storer(key).attrs.metadata = {
                        'spot': surface_df.spot,
                        'valuation_date': surface_df.valuation_date
                    }


