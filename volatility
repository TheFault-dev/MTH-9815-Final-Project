 import pandas as pd
import numpy as np

def get_constant_maturity_iv(tickers, start_date, end_date, maturity=30/365.25):
    """
    Generates a time series of constant maturity ATM implied volatility for multiple tickers.

    Args:
        tickers (list): List of stock tickers.
        start_date (str): Start date in 'YYYY-MM-DD' format.
        end_date (str): End date in 'YYYY-MM-DD' format.
        maturity (float): Desired constant maturity in years (e.g., 30 days = 30/365.25).

    Returns:
        pd.DataFrame: A DataFrame with dates as the index and IV time series as columns.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    all_ivs = {}

    for ticker in tickers:
        ticker_ivs = []
        for date in dates:
            try:
                surface = get_iv_surface(ticker, date) # User's provided function
                expiries = surface['Expiry'].values
                atm_vols = surface['1.0'].values
                
                # Linear interpolation to find the constant maturity IV
                constant_iv = np.interp(maturity, expiries, atm_vols)
                ticker_ivs.append(constant_iv)
            except Exception as e:
                ticker_ivs.append(np.nan) # Handle cases where data is not available
        all_ivs[ticker] = ticker_ivs
        
    df_iv = pd.DataFrame(all_ivs, index=dates)
    df_iv.dropna(how='all', inplace=True)
    return df_iv

# Example Usage:
# spy_components = spy_weights['CMDB Ticker'].tolist()
# iv_timeseries = get_constant_maturity_iv(spy_components, '2022-01-01', '2023-12-31')


def analyze_iv_comovement(df_iv, benchmark_ticker, stress_threshold=0.02):
    """
    Analyzes the co-movement of implied volatilities against a benchmark.

    Args:
        df_iv (pd.DataFrame): Time series of implied volatilities.
        benchmark_ticker (str): The ticker to use as the benchmark (e.g., 'SPY-US').
        stress_threshold (float): The daily percentage drop in benchmark to define a stress day.

    Returns:
        dict: A dictionary containing correlation and beta matrices for all, stress, and regular periods.
    """
    iv_returns = df_iv.pct_change().dropna()
    benchmark_returns = iv_returns[benchmark_ticker]
    
    # Identify stress periods
    stress_days = benchmark_returns < -stress_threshold
    regular_days = ~stress_days

    # Calculate correlation
    corr_all = iv_returns.corr()
    corr_stress = iv_returns[stress_days].corr()
    corr_regular = iv_returns[regular_days].corr()

    # Calculate scaling effect (beta)
    betas = iv_returns.apply(lambda x: np.polyfit(benchmark_returns, x, 1)[0])
    
    return {
        "correlation_all": corr_all,
        "correlation_stress": corr_stress,
        "correlation_regular": corr_regular,
        "scaling_betas": betas
    }

# Example Usage:
# comovement_results = analyze_iv_comovement(iv_timeseries, 'SPY-US')
# print("Scaling Betas:\n", comovement_results['scaling_betas'])


from arch import arch_model

def calculate_vol_of_vol(df_iv):
    """
    Calculates the volatility of implied volatility using three different methods.

    Args:
        df_iv (pd.DataFrame): Time series of implied volatilities.

    Returns:
        dict: A dictionary of DataFrames for each vol-of-vol calculation method.
    """
    iv_returns = df_iv.pct_change().dropna()
    
    # 1. 21-Day Rolling Window
    vol_rolling = iv_returns.rolling(window=21).std() * np.sqrt(252)

    # 2. EWMA
    vol_ewma = iv_returns.ewm(span=21, adjust=False).std() * np.sqrt(252)
    
    # 3. EGARCH(1,1)
    def fit_egarch(series):
        model = arch_model(series * 100, p=1, o=1, q=1, vol='EGARCH')
        res = model.fit(disp='off')
        return res.conditional_volatility / 100
        
    vol_egarch = iv_returns.apply(fit_egarch, axis=0) * np.sqrt(252)

    return {
        "rolling_21d": vol_rolling,
        "ewma_21d": vol_ewma,
        "egarch_1_1": vol_egarch
    }

# Example Usage:
# vol_of_vol_results = calculate_vol_of_vol(iv_timeseries)
# print("EWMA Vol of Vol:\n", vol_of_vol_results['ewma_21d'].tail())



from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

def verify_power_law_decay(ticker, valuation_date):
    """
    Fits a power law to the IV term structure using L2 regularized regression.
    The model is IV = c + b1*T^(-0.5) + b2*T^(-1.0), fit via log-log regression.

    Args:
        ticker (str): The stock ticker.
        valuation_date (str): The valuation date 'YYYY-MM-DD'.

    Returns:
        dict: A dictionary with the regression model and its parameters.
    """
    surface = get_iv_surface(ticker, pd.to_datetime(valuation_date))
    surface = surface.loc[surface['Expiry'] > 0]
    
    X = np.log(surface[['Expiry']].values)
    y = np.log(surface['1.0'].values)
    
    # Using Ridge (L2 regularization) on a log-log plot to find the power
    model = make_pipeline(PolynomialFeatures(1), Ridge(alpha=1.0))
    model.fit(X, y)
    
    # The coefficient of the log(T) term is the power 'alpha'
    # log(IV) = log(k) + alpha * log(T)
    alpha = model.named_steps['ridge'].coef_[1]
    
    return {
        "model": model,
        "power_law_exponent (alpha)": alpha
    }

# Example Usage:
# power_law_fit = verify_power_law_decay('AAPL-UQ', '2023-06-30')
# print(f"Fitted Power Law Exponent: {power_law_fit['power_law_exponent (alpha)']:.4f}")



def backtest_beta_hedging(portfolio_tickers, hedge_instrument, df_iv, betas):
    """
    Runs a simple backtest for a beta-weighted Vega hedge.

    Args:
        portfolio_tickers (list): List of tickers in the long portfolio.
        hedge_instrument (str): The ticker for the hedging instrument (e.g., 'SPY-US').
        df_iv (pd.DataFrame): Time series of implied volatilities.
        betas (pd.Series): Series of scaling betas against the hedge instrument.

    Returns:
        pd.DataFrame: A DataFrame showing the unhedged vs. hedged portfolio IV changes.
    """
    iv_returns = df_iv.pct_change().dropna()

    # Assume an equally-weighted portfolio (long 1 unit of IV in each stock)
    portfolio_iv_returns = iv_returns[portfolio_tickers].mean(axis=1)

    # Calculate the portfolio's aggregate beta
    portfolio_beta = betas[portfolio_tickers].mean()
    
    # The optimal hedge ratio is the portfolio's beta
    # We short 'portfolio_beta' units of the hedge instrument's IV
    hedge_iv_returns = portfolio_beta * iv_returns[hedge_instrument]
    
    # Calculate hedged portfolio returns
    hedged_portfolio_iv_returns = portfolio_iv_returns - hedge_iv_returns
    
    # Create results DataFrame
    results = pd.DataFrame({
        'unhedged_iv_change': portfolio_iv_returns,
        'hedged_iv_change': hedged_portfolio_iv_returns
    })
    
    print(f"Unhedged Volatility: {results['unhedged_iv_change'].std() * 100:.4f}%")
    print(f"Hedged Volatility:   {results['hedged_iv_change'].std() * 100:.4f}%")
    
    return results

# Example Usage:
# Assume we ran Module 2 and got the betas
# betas = comovement_results['scaling_betas']
# my_portfolio = ['AAPL-UQ', 'MSFT-US', 'NVDA-US'] # Our proxy portfolio
# hedge_results = backtest_beta_hedging(my_portfolio, 'SPY-US', iv_timeseries, betas)


import pandas as pd
import numpy as np

def store_vol_surfaces_to_hdf(spy_weights_df, start_date, end_date, output_path='vol_surfaces.h5'):
    """
    Fetches and stores IV surfaces in an HDF5 file with string-formatted columns.
    """
    top_50_tickers = spy_weights_df.nlargest(50, 'ID().WEIGHTS')['CMDB Ticker'].tolist()
    all_tickers = list(set(top_50_tickers + ['SPY-UP', 'SPX']))
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))

    with pd.HDFStore(output_path, mode='w', complevel=9, complib='blosc') as store:
        for ticker in all_tickers:
            for date in dates:
                # This is your provided function to get the raw surface
                surface_df = get_iv_surface(ticker, date)

                # Convert numeric columns to valid string names (e.g., 1.0 -> 'm_1_0')
                rename_map = {c: f"m_{str(c).replace('.', '_')}" for c in surface_df.columns if isinstance(c, (int, float))}
                surface_df.rename(columns=rename_map, inplace=True)
                
                # Create a safe key for HDF5 storage
                key = f'/{ticker.replace("-", "_").replace(".", "")}/{date.strftime("d%Y%m%d")}'
                
                # Store the DataFrame
                store.put(key, surface_df, format='table')
                
                # Store metadata
                store.get_storer(key).attrs.metadata = {
                    'spot': surface_df.spot,
                    'valuation_date': surface_df.valuation_date
                }

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity=30/365.25):
    """
    Calculates fixed-strike IV changes using the corrected HDF5 file.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    iv_changes = {}

    with pd.HDFStore(db_path, mode='r') as store:
        for ticker in tickers:
            ticker_key_name = ticker.replace("-", "_").replace(".", "")
            prev_atm_iv = np.nan
            prev_spot = np.nan
            daily_changes = []

            for date in dates:
                key = f'/{ticker_key_name}/{date.strftime("d%Y%m%d")}'
                if key not in store.keys():
                    daily_changes.append(np.nan)
                    continue

                surface = store[key]
                metadata = store.get_storer(key).attrs.metadata
                spot_today = metadata['spot']
                
                iv_cols = [c for c in surface.columns if c.startswith('m_')]
                moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
                
                # Sort them to ensure they are in ascending order for interpolation
                sorted_indices = np.argsort(moneyness_levels)
                moneyness_levels = np.array(moneyness_levels)[sorted_indices]
                iv_surface_values = surface[np.array(iv_cols)[sorted_indices]].values
                
                expiries = surface['Expiry'].values
                constant_maturity_vols = np.apply_along_axis(
                    lambda vol_col: np.interp(maturity, expiries, vol_col), 0, iv_surface_values
                )

                if not np.isnan(prev_spot):
                    moneyness_of_prev_strike = prev_spot / spot_today
                    iv_of_prev_strike_today = np.interp(
                        moneyness_of_prev_strike, moneyness_levels, constant_maturity_vols
                    )
                    change = iv_of_prev_strike_today - prev_atm_iv
                    daily_changes.append(change)
                else:
                    daily_changes.append(np.nan)

                current_atm_iv = np.interp(1.0, moneyness_levels, constant_maturity_vols)
                prev_atm_iv = current_atm_iv
                prev_spot = spot_today

            iv_changes[ticker] = daily_changes

    results_df = pd.DataFrame(iv_changes, index=dates)
    return results_df.dropna(how='all')

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity=30/365.25):
    """
    Calculates fixed-strike IV changes using the corrected HDF5 file.
    """
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))
    iv_changes = {}

    with pd.HDFStore(db_path, mode='r') as store:
        for ticker in tickers:
            ticker_key_name = ticker.replace("-", "_").replace(".", "")
            prev_atm_iv = np.nan
            prev_spot = np.nan
            daily_changes = []

            for date in dates:
                key = f'/{ticker_key_name}/{date.strftime("d%Y%m%d")}'
                if key not in store.keys():
                    daily_changes.append(np.nan)
                    continue

                surface = store[key]
                metadata = store.get_storer(key).attrs.metadata
                spot_today = metadata['spot']
                
                iv_cols = [c for c in surface.columns if c.startswith('m_')]
                moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
                
                # Sort them to ensure they are in ascending order for interpolation
                sorted_indices = np.argsort(moneyness_levels)
                moneyness_levels = np.array(moneyness_levels)[sorted_indices]
                iv_surface_values = surface[np.array(iv_cols)[sorted_indices]].values
                
                expiries = surface['Expiry'].values
                constant_maturity_vols = np.apply_along_axis(
                    lambda vol_col: np.interp(maturity, expiries, vol_col), 0, iv_surface_values
                )

                if not np.isnan(prev_spot):
                    moneyness_of_prev_strike = prev_spot / spot_today
                    iv_of_prev_strike_today = np.interp(
                        moneyness_of_prev_strike, moneyness_levels, constant_maturity_vols
                    )
                    change = iv_of_prev_strike_today - prev_atm_iv
                    daily_changes.append(change)
                else:
                    daily_changes.append(np.nan)

                current_atm_iv = np.interp(1.0, moneyness_levels, constant_maturity_vols)
                prev_atm_iv = current_atm_iv
                prev_spot = spot_today

            iv_changes[ticker] = daily_changes

    results_df = pd.DataFrame(iv_changes, index=dates)
    return results_df.dropna(how='all')

# 9:30
import pandas as pd

# The path to the HDF5 file you already created
db_path = 'vol_surfaces.h5'

print(f"Inspecting structure of: {db_path}\n" + "="*40)

with pd.HDFStore(db_path, mode='r') as store:
    # Get all the keys (paths) in the file
    all_keys = store.keys()
    
    # Print the first 20 keys to see the structure
    print("First 20 keys in the file:")
    for key in all_keys[:20]:
        print(key)
        
    print(f"\nTotal items in file: {len(all_keys)}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Load All Data for One Stock into Memory ---

# Choose a single stock to process for this test
ticker_to_process = 'AAPL-UQ' 
ticker_key_name = ticker_to_process.replace("-", "_").replace(".", "")
maturity_in_years = 30 / 365.25

# This dictionary will hold all the data we load from the file
# Keys will be dates, values will be another dict: {'surface': df, 'spot': spot_price}
stock_data = {}

# Open the file ONCE to pull all data for our chosen stock
with pd.HDFStore(db_path, mode='r') as store:
    for key in store.keys():
        if f'/{ticker_key_name}/' in key:
            # Extract the date from the key string
            date_str = key.split('/')[-1].replace('d', '')
            date = pd.to_datetime(date_str, format='%Y%m%d')
            
            # Load the surface and its metadata into our dictionary
            surface_df = store[key]
            metadata = store.get_storer(key).attrs.metadata
            stock_data[date] = {'surface': surface_df, 'spot': metadata['spot']}

# Convert the dictionary into a DataFrame for easy processing
df = pd.DataFrame.from_dict(stock_data, orient='index').sort_index()


# --- Step 2: Calculate the IV Change Series using Pandas ---

# Create columns for the PREVIOUS day's data using .shift()
df['prev_spot'] = df['spot'].shift(1)
df['prev_surface'] = df['surface'].shift(1)

# This function calculates the change for a single row (one day)
def calculate_iv_change_for_row(row):
    # Skip the first row or any day missing previous data
    if pd.isna(row['prev_spot']) or row['prev_surface'] is None:
        return np.nan

    # --- Get previous day's ATM IV ---
    prev_surface = row['prev_surface']
    
    # Because of the 'm_x_y' format, we must parse them back to numbers
    iv_cols = [c for c in prev_surface.columns if c.startswith('m_')]
    moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
    
    # The string format requires sorting to ensure interpolation works correctly
    sorted_indices = np.argsort(moneyness_levels)
    moneyness_levels = np.array(moneyness_levels)[sorted_indices]
    prev_iv_surface_values = prev_surface[np.array(iv_cols)[sorted_indices]].values
    
    # Interpolate to get the 30-day constant maturity slice
    prev_expiries = prev_surface['Expiry'].values
    prev_constant_maturity_vols = np.apply_along_axis(
        lambda vol_col: np.interp(maturity_in_years, prev_expiries, vol_col), 0, prev_iv_surface_values
    )
    
    # Find the ATM IV from the previous day
    prev_atm_iv = np.interp(1.0, moneyness_levels, prev_constant_maturity_vols)

    # --- Get today's IV for yesterday's strike ---
    today_surface = row['surface']
    today_spot = row['spot']
    
    # Repeat the parsing for today's surface
    iv_cols = [c for c in today_surface.columns if c.startswith('m_')]
    moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
    sorted_indices = np.argsort(moneyness_levels)
    moneyness_levels = np.array(moneyness_levels)[sorted_indices]
    today_iv_surface_values = today_surface[np.array(iv_cols)[sorted_indices]].values

    today_expiries = today_surface['Expiry'].values
    today_constant_maturity_vols = np.apply_along_axis(
        lambda vol_col: np.interp(maturity_in_years, today_expiries, vol_col), 0, today_iv_surface_values
    )
    
    # Calculate moneyness of yesterday's strike on today's surface
    moneyness_of_prev_strike = row['prev_spot'] / today_spot
    
    # Interpolate to find the IV
    iv_of_prev_strike_today = np.interp(moneyness_of_prev_strike, moneyness_levels, today_constant_maturity_vols)
    
    # The final result for the day
    return iv_of_prev_strike_today - prev_atm_iv

# Apply the function to every row to calculate the series
df['iv_change'] = df.apply(calculate_iv_change_for_row, axis=1)


# --- Step 3: Visualize the Resulting Series ---

plt.style.use('seaborn-v0_8-grid')
fig, ax = plt.subplots(figsize=(15, 7))

df['iv_change'].plot(ax=ax, label='Daily IV Change', marker='.', linestyle='-')

ax.set_title(f'Calculated Fixed-Strike IV Change Series for {ticker_to_process}', fontsize=16)
ax.set_xlabel('Date', fontsize=12)
ax.set_ylabel('Absolute Change in Implied Volatility', fontsize=12)
ax.axhline(0, color='red', linestyle='--', lw=1)
ax.legend()
plt.tight_layout()
plt.show()

# Display the head and tail of the final data
print(f"\nFinal DataFrame with calculated changes for {ticker_to_process}:\n" + "="*60)
print(df[['spot', 'prev_spot', 'iv_change']].dropna().head())
print("...")
print(df[['spot', 'prev_spot', 'iv_change']].dropna().tail())

# 9:45

import pandas as pd
import numpy as np

def calculate_fixed_strike_iv_changes(tickers, start_date, end_date, db_path='vol_surfaces.h5', maturity_in_years=30/365.25):
    
    all_results = {}

    def calculate_iv_change_for_row(row):
        if pd.isna(row['prev_spot']) or row['prev_surface'] is None:
            return np.nan

        prev_surface = row['prev_surface']
        iv_cols = [c for c in prev_surface.columns if c.startswith('m_')]
        moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
        sorted_indices = np.argsort(moneyness_levels)
        moneyness_levels = np.array(moneyness_levels)[sorted_indices]
        prev_iv_surface_values = prev_surface[np.array(iv_cols)[sorted_indices]].values
        prev_expiries = prev_surface['Expiry'].values
        prev_constant_maturity_vols = np.apply_along_axis(
            lambda vol_col: np.interp(maturity_in_years, prev_expiries, vol_col), 0, prev_iv_surface_values
        )
        prev_atm_iv = np.interp(1.0, moneyness_levels, prev_constant_maturity_vols)

        today_surface = row['surface']
        today_spot = row['spot']
        iv_cols = [c for c in today_surface.columns if c.startswith('m_')]
        moneyness_levels = [float(c.replace('m_', '').replace('_', '.')) for c in iv_cols]
        sorted_indices = np.argsort(moneyness_levels)
        moneyness_levels = np.array(moneyness_levels)[sorted_indices]
        today_iv_surface_values = today_surface[np.array(iv_cols)[sorted_indices]].values
        today_expiries = today_surface['Expiry'].values
        today_constant_maturity_vols = np.apply_along_axis(
            lambda vol_col: np.interp(maturity_in_years, today_expiries, vol_col), 0, today_iv_surface_values
        )

        moneyness_of_prev_strike = row['prev_spot'] / today_spot
        iv_of_prev_strike_today = np.interp(moneyness_of_prev_strike, moneyness_levels, today_constant_maturity_vols)

        return iv_of_prev_strike_today - prev_atm_iv

    with pd.HDFStore(db_path, mode='r') as store:
        all_file_keys = store.keys()
        
        for ticker in tickers:
            ticker_key_name = f"/{ticker.replace('-', '_').replace('.', '')}/"
            stock_data = {}

            keys_for_ticker = [key for key in all_file_keys if key.startswith(ticker_key_name)]

            for key in keys_for_ticker:
                date_str = key.split('/')[-1].replace('d', '')
                date = pd.to_datetime(date_str, format='%Y%m%d')
                
                surface_df = store.get(key)
                metadata = store.get_storer(key).attrs.metadata
                stock_data[date] = {'surface': surface_df, 'spot': metadata['spot']}
            
            if not stock_data:
                continue

            df = pd.DataFrame.from_dict(stock_data, orient='index').sort_index()
            df = df.loc[start_date:end_date]
            
            if df.empty:
                continue

            df['prev_spot'] = df['spot'].shift(1)
            df['prev_surface'] = df['surface'].shift(1)
            
            all_results[ticker] = df.apply(calculate_iv_change_for_row, axis=1)

    final_df = pd.DataFrame(all_results)
    return final_df

# 10:20

import pandas as pd

# --- Configuration ---
db_path = 'vol_surfaces.h5'
date_to_check = '2023-01-03' # Pick any date that exists in your dataset
ticker1 = 'AAPL-UQ'
ticker2 = 'MSFT-US' # Pick two different tickers from your top 50 list

# --- Debugging Code ---
print("--- HDF5 Data Integrity Check ---")

# Construct the precise keys for the HDF5 file
key1 = f"/{ticker1.replace('-', '_').replace('.', '')}/{pd.to_datetime(date_to_check).strftime('d%Y%m%d')}"
key2 = f"/{ticker2.replace('-', '_').replace('.', '')}/{pd.to_datetime(date_to_check).strftime('d%Y%m%d')}"

with pd.HDFStore(db_path, mode='r') as store:
    # Verify both keys actually exist in the file
    if key1 not in store or key2 not in store:
        print(f"Error: A key for date {date_to_check} is missing for one of the tickers.")
    else:
        # Load the data and metadata for both tickers
        df1 = store.get(key1)
        meta1 = store.get_storer(key1).attrs.metadata
        
        df2 = store.get(key2)
        meta2 = store.get_storer(key2).attrs.metadata

        # Print the spot price and head of each DataFrame
        print(f"\nData for {ticker1} on {date_to_check}:")
        print(f"Spot Price: {meta1['spot']}")
        print(df1.head(3))
        
        print(f"\nData for {ticker2} on {date_to_check}:")
        print(f"Spot Price: {meta2['spot']}")
        print(df2.head(3))

        # Programmatically check if the two DataFrames are identical
        are_they_equal = df1.equals(df2)
        
        print("\n--- Verification Result ---")
        if are_they_equal:
            print("🔴 CRITICAL FLAW CONFIRMED: The data saved for both tickers is identical.")
        else:
            print("🟢 The data for the two tickers is different, as expected.")


# 10:30

import pandas as pd
import numpy as np

def store_vol_surfaces_to_hdf(spy_weights_df, start_date, end_date, output_path='vol_surfaces.h5'):
    
    top_50_tickers = spy_weights_df.nlargest(50, 'ID().WEIGHTS')['CMDB Ticker'].tolist()
    all_tickers = list(set(top_50_tickers + ['SPY-UP', 'SPX']))
    dates = pd.to_datetime(pd.date_range(start_date, end_date, freq='B'))

    with pd.HDFStore(output_path, mode='w', complevel=9, complib='blosc') as store:
        for ticker in all_tickers:
            for date in dates:
                surface_df = get_iv_surface(ticker, date)

                if surface_df is not None:
                    rename_map = {c: f"m_{str(c).replace('.', '_')}" for c in surface_df.columns if isinstance(c, (int, float))}
                    
                    # --- THE FIX: Avoid inplace=True and reassign the DataFrame ---
                    surface_df = surface_df.rename(columns=rename_map)
                    
                    key = f'/{ticker.replace("-", "_").replace(".", "")}/{date.strftime("d%Y%m%d")}'
                    
                    store.put(key, surface_df, format='table')
                    
                    store.get_storer(key).attrs.metadata = {
                        'spot': surface_df.spot,
                        'valuation_date': surface_df.valuation_date
                    }

# 16:30

import matplotlib.pyplot as plt
import seaborn as sns

def plot_iv_series(iv_change_series, benchmarks):
    plt.style.use('seaborn-v0_8-grid')
    fig, ax = plt.subplots(figsize=(18, 9))

    for ticker in iv_change_series.columns:
        if ticker in benchmarks:
            ax.plot(iv_change_series.index, iv_change_series[ticker], lw=2.0, label=ticker)
        else:
            ax.plot(iv_change_series.index, iv_change_series[ticker], lw=0.7, linestyle='--')

    ax.set_title('Implied Volatility Change Series', fontsize=16)
    ax.set_xlabel('Date', fontsize=12)
    ax.set_ylabel('Absolute Change in IV', fontsize=12)
    ax.legend()
    plt.tight_layout()
    plt.show()

# Example Usage:
# benchmarks = ['SPY-UP', 'SPX'] 
# plot_iv_series(iv_change_series, benchmarks)


import pandas as pd
import numpy as np

# Example Usage:
# dispersion_betas = calculate_dispersion_matrix(iv_change_series, benchmarks)


from statsmodels.tsa.stattools import adfuller

def analyze_stationarity_and_drift(iv_change_series):
    results = []
    for ticker in iv_change_series.columns:
        series = iv_change_series[ticker].dropna()
        adf_result = adfuller(series)
        
        # Decision: 'Drift' is interpreted as the mean of the daily IV change series.
        drift = series.mean() * 252 # Annualized drift
        
        results.append({
            'Ticker': ticker,
            'ADF_p_value': adf_result[1],
            'Is_Stationary_at_5%': adf_result[1] < 0.05,
            'Annualized_Drift': drift
        })

    results_df = pd.DataFrame(results).set_index('Ticker')
    results_df = results_df.sort_values(by='Annualized_Drift', ascending=False)
    return results_df

# Example Usage:
# stationarity_report = analyze_stationarity_and_drift(iv_change_series)
# print(stationarity_report)


from arch import arch_model

def analyze_volatility_clustering(iv_change_series):
    garch_results = []
    for ticker in iv_change_series.columns:
        series = iv_change_series[ticker].dropna() * 100
        model = arch_model(series, vol='Garch', p=1, q=1, dist='ged')
        res = model.fit(disp='off')
        
        garch_results.append({
            'Ticker': ticker,
            'alpha[1]': res.params['alpha[1]'],
            'beta[1]': res.params['beta[1]'],
            'alpha+beta': res.params['alpha[1]'] + res.params['beta[1]']
        })

    garch_df = pd.DataFrame(garch_results).set_index('Ticker').sort_values('alpha+beta', ascending=False)
    print("--- GARCH Volatility Clustering Results ---")
    print(garch_df.head(10))

    corr_matrix = iv_change_series.corr()
    
    # Decision: Use seaborn's clustermap to automatically group similar stocks.
    sns.clustermap(corr_matrix, cmap='viridis', annot=False, figsize=(14, 14))
    plt.suptitle('Hierarchical Clustering of IV Change Correlation', y=1.02, fontsize=16)
    plt.show()
    
    return garch_df, corr_matrix

# Example Usage:
# garch_report, correlation_matrix = analyze_volatility_clustering(iv_change_series)


def vol_of_vol_predictive_model(iv_change_series, base_ticker, target_ticker):
    base_series = iv_change_series[base_ticker].dropna() * 100
    target_series = iv_change_series[target_ticker].dropna() * 100
    
    # Align the series
    df = pd.DataFrame({'base': base_series, 'target': target_series}).dropna()

    # Fit GARCH to both to get conditional vol series
    base_garch = arch_model(df['base'], vol='Garch', p=1, q=1).fit(disp='off')
    target_garch = arch_model(df['target'], vol='Garch', p=1, q=1).fit(disp='off')
    
    vols_df = pd.DataFrame({
        'base_vol': base_garch.conditional_volatility,
        'target_vol': target_garch.conditional_volatility
    })
    
    # Predictive regression: Target Vol(t) = C + Beta * Base Vol(t)
    y = vols_df['target_vol']
    X = sm.add_constant(vols_df['base_vol']) # Using statsmodels for a clean summary
    
    model = sm.OLS(y, X).fit()
    print(f"\n--- Predictive Model: {target_ticker} Vol vs. {base_ticker} Vol ---")
    print(model.summary())
    
    # Visualize the relationship
    sns.regplot(x='base_vol', y='target_vol', data=vols_df, line_kws={'color': 'red'})
    plt.title(f'Conditional Volatility Relationship: {base_ticker} vs. {target_ticker}', fontsize=16)
    plt.xlabel(f'{base_ticker} Conditional Volatility')
    plt.ylabel(f'{target_ticker} Conditional Volatility')
    plt.show()

# Example Usage:
# import statsmodels.api as sm
# base_ticker = 'SPY-UP'
# target_ticker = 'AAPL-UQ' # Choose any stock from your list
# vol_of_vol_predictive_model(iv_change_series, base_ticker, target_ticker)


# 10:35


import pandas as pd
import numpy as np

def winsorize_and_report_volatility(iv_change_series, method='sigma', threshold=3):
    
    original_vol = iv_change_series.std().sort_values(ascending=False)
    print("--- Top 10 Most Volatile Series (Pre-Winsorization) ---")
    print(original_vol.head(10))

    winsorized_df = iv_change_series.copy()

    for col in winsorized_df.columns:
        series = winsorized_df[col].dropna()

        if method == 'sigma':
            mean = series.mean()
            std_dev = series.std()
            upper_bound = mean + threshold * std_dev
            lower_bound = mean - threshold * std_dev
        
        elif method == 'mad':
            median = series.median()
            # Scaled MAD for comparison with standard deviation
            mad = (series - median).abs().median() * 1.4826 
            upper_bound = median + threshold * mad
            lower_bound = median - threshold * mad
        
        else:
            return None

        winsorized_df[col] = winsorized_df[col].clip(lower=lower_bound, upper=upper_bound)

    return winsorized_df

# Example Usage:
# winsorized_iv_changes = winsorize_and_report_volatility(iv_change_series, method='mad', threshold=3.5)


import matplotlib.pyplot as plt
import seaborn as sns

def analyze_correlation_stability(iv_change_series, ticker1, ticker2, window=60):
    
    static_corr = iv_change_series.corr()
    
    print(f"--- Static Correlation for {ticker1} and {ticker2}: {static_corr.loc[ticker1, ticker2]:.4f} ---")
    
    rolling_corr = iv_change_series[ticker1].rolling(window=window).corr(iv_change_series[ticker2])
    
    fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True, gridspec_kw={'height_ratios': [2, 1]})
    
    axes[0].plot(iv_change_series.index, iv_change_series[ticker1], label=ticker1, lw=1.0)
    axes[0].plot(iv_change_series.index, iv_change_series[ticker2], label=ticker2, lw=1.0)
    axes[0].set_title(f'IV Change Series: {ticker1} vs. {ticker2}', fontsize=14)
    axes[0].set_ylabel('IV Change')
    axes[0].legend()
    
    axes[1].plot(rolling_corr.index, rolling_corr, label=f'{window}-Day Rolling Correlation', color='red')
    axes[1].axhline(static_corr.loc[ticker1, ticker2], color='black', linestyle='--', lw=1.5, label='Static Correlation')
    axes[1].set_title('Correlation Stability Analysis', fontsize=14)
    axes[1].set_ylabel('Correlation')
    axes[1].set_xlabel('Date')
    axes[1].legend()
    
    plt.tight_layout()
    plt.show()

    return static_corr, rolling_corr

# Example Usage:
# Choose two tickers to compare, for example a tech giant and a benchmark.
# static_corr_matrix, rolling_corr_series = analyze_correlation_stability(iv_change_series, 'AAPL-UQ', 'SPY-UP', window=90)



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_all_pairs_correlation(iv_change_series, benchmarks, window=60):
    
    non_benchmark_tickers = [t for t in iv_change_series.columns if t not in benchmarks]
    df = iv_change_series[non_benchmark_tickers]
    
    # --- Static Correlation Matrix and Heatmap ---
    static_corr_matrix = df.corr()
    
    plt.figure(figsize=(16, 14))
    sns.heatmap(static_corr_matrix, cmap='viridis', annot=False)
    plt.title('Static Pairwise Correlation Matrix', fontsize=16)
    plt.show()

    # --- Rolling Correlation Stability Analysis ---
    results = []
    
    for i in range(len(df.columns)):
        for j in range(i + 1, len(df.columns)):
            ticker1 = df.columns[i]
            ticker2 = df.columns[j]
            
            rolling_corr = df[ticker1].rolling(window=window).corr(df[ticker2]).dropna()
            
            if not rolling_corr.empty:
                results.append({
                    'pair': f"{ticker1}-{ticker2}",
                    'mean_corr': rolling_corr.mean(),
                    'std_dev_corr': rolling_corr.std()
                })

    corr_stability_df = pd.DataFrame(results)
    
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(12, 8))
    
    sns.scatterplot(
        data=corr_stability_df,
        x='mean_corr',
        y='std_dev_corr',
        hue='mean_corr',
        size='std_dev_corr',
        sizes=(20, 200),
        palette='viridis',
        legend=False
    )
    
    plt.title('Pairwise Correlation Stability Map', fontsize=16)
    plt.xlabel('Mean Rolling Correlation', fontsize=12)
    plt.ylabel('Standard Deviation of Rolling Correlation (Volatility)', fontsize=12)

    most_stable_pair = corr_stability_df.sort_values(by=['mean_corr', 'std_dev_corr'], ascending=[False, True]).iloc[0]
    plt.text(most_stable_pair['mean_corr'], most_stable_pair['std_dev_corr'], f"  Most Stable:\n  {most_stable_pair['pair']}", ha='left')

    plt.tight_layout()
    plt.show()

    return static_corr_matrix, corr_stability_df

# Example Usage:
# benchmarks = ['SPY-UP', 'SPX']
# static_matrix, stability_report = analyze_all_pairs_correlation(iv_change_series, benchmarks, window=90)
# print("\n--- Top 10 Most Stable & Correlated Pairs ---")
# print(stability_report.sort_values(by='mean_corr', ascending=False).head(10))


# 2:25

import numpy as np
from numpy.linalg import inv, pinv

def hedge_weights(z, w, A, Sigma, x, use_pinv=False):
    """
    Returns optimal hedge weights u* that minimise E[(ΔP)^2].
    
    Parameters
    ----------
    z      : (n,)    dollar vega 1% per single name
    w      : (n,)    portfolio weights
    A      : (n,m)   dispersion matrix
    Sigma  : (n,n)   vol-move correlation / covariance
    x      : (m,)    dollar vega 1% per index instrument
    use_pinv : bool  set True to fall back on Moore-Penrose
    
    Returns
    -------
    u_star : (m,)    contracts (or vega notional / x) for each hedge
    h_star : (m,)    dollar vega per 1% change in each index
    """
    q = z * w                            # element-wise
    lhs = A.T @ Sigma @ A
    rhs = A.T @ Sigma @ q
    solver = pinv if use_pinv else inv
    h_star = solver(lhs) @ rhs
    u_star = h_star / x                  # element-wise
    return u_star, h_star

n = 10
m = 3
z      = np.full(n, 1e5)         # $100k each
w      = np.full(n, 1/n)         # equal weights
A      = np.random.uniform(0.3, 0.9, size=(n, m))   # toy dispersion
Sigma  = np.eye(n)               # uncorrelated vols for demo
x      = np.full(m, 1e6)         # say $1 M vega per index option

u_star, h_star = hedge_weights(z, w, A, Sigma, x)
print("Index-contract hedge weights  u*:", u_star)
print("Dollar-vega exposures         h*:", h_star)

# 2:40

import pandas as pd
import numpy as np

def calculate_gls_optimal_hedge(
    portfolio_weights,
    vega_1_percent_stocks,
    vega_1_percent_hedges,
    dispersion_matrix,
    iv_change_series
):
    
    stock_tickers = portfolio_weights.index
    hedge_tickers = vega_1_percent_hedges.index

    q_dollar_vega = portfolio_weights * vega_1_percent_stocks
    
    # Calculate the covariance matrix Sigma from the time series
    Sigma = iv_change_series[stock_tickers].cov()
    
    # Align matrices to the portfolio's stocks and hedges
    A = dispersion_matrix.loc[stock_tickers, hedge_tickers].values
    q = q_dollar_vega.values
    
    # Implement the GLS formula: h = (A_T * Sigma * A)^-1 * (A_T * Sigma * q)
    A_T_Sigma = A.T @ Sigma
    A_T_Sigma_A = A_T_Sigma @ A
    
    # Use the Moore-Penrose pseudo-inverse for numerical stability
    A_T_Sigma_A_inv = np.linalg.pinv(A_T_Sigma_A)
    
    h_star = A_T_Sigma_A_inv @ A_T_Sigma @ q
    
    # Convert dollar-vega amounts (h) back to contract weights (u)
    u_star = h_star / vega_1_percent_hedges.values

    results = pd.DataFrame({
        'Hedge_Dollar_Vega_h': h_star,
        'Hedge_Contract_Weights_u': u_star
    }, index=hedge_tickers)
    
    return results

# --- Test Case ---



import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

# --- Setup from existing data ---
# Assume 'iv_change_series' is your DataFrame of IV changes.
# Assume 'benchmarks' is your list of hedge instrument tickers.

stock_tickers = [t for t in iv_change_series.columns if t not in benchmarks]
hedge_tickers = benchmarks

# --- Define Portfolio and Vega Assumptions ---
portfolio_weights = pd.Series(1/len(stock_tickers), index=stock_tickers)
vega_1_percent_stocks = pd.Series(100000.0, index=stock_tickers)

# Decision: Assume a Vega 1% for the hedging instruments.
vega_1_percent_hedges = pd.Series({
    'SPY-UP': 25000.0, 
    'SPX': 35000.0
}, index=hedge_tickers)


# --- Calculate Dispersion Matrix 'A' from Time Series ---
# Based on the model Δτ = AᵀΔσ, we regress each benchmark's IV change 
# against all of the stock IV changes to find the coefficients that form Aᵀ.

df_clean = iv_change_series.dropna()
X_train = df_clean[stock_tickers]
A_T_list = []

for ticker in hedge_tickers:
    y_train = df_clean[ticker]
    model = LinearRegression(fit_intercept=False).fit(X_train, y_train)
    A_T_list.append(model.coef_)

A_T = np.array(A_T_list)
A = pd.DataFrame(A_T.T, index=stock_tickers, columns=hedge_tickers)


# --- Calculate the Optimal Hedge ---
optimal_hedge = calculate_gls_optimal_hedge(
    portfolio_weights=portfolio_weights,
    vega_1_percent_stocks=vega_1_percent_stocks,
    vega_1_percent_hedges=vega_1_percent_hedges,
    dispersion_matrix=A,
    iv_change_series=iv_change_series
)


# --- Print Final Result ---
print("--- GLS Optimal Hedge Weights ---\n")
print(optimal_hedge.round(4))

# 3:00


from sklearn.linear_model import LinearRegression

def calculate_and_plot_gls_dispersion(iv_change_series, benchmarks):

    stock_tickers = [t for t in iv_change_series.columns if t not in benchmarks]
    df_clean = iv_change_series.dropna()
    
    X_train = df_clean[stock_tickers]
    A_T_list = []

    for ticker in benchmarks:
        y_train = df_clean[ticker]
        model = LinearRegression(fit_intercept=False).fit(X_train, y_train)
        A_T_list.append(model.coef_)

    A_T = np.array(A_T_list)
    A_matrix = pd.DataFrame(A_T.T, index=stock_tickers, columns=benchmarks)
    
    # Sort by the dispersion to the first benchmark for clearer visualization
    A_matrix_sorted = A_matrix.sort_values(by=benchmarks[0], ascending=False)
    
    plt.figure(figsize=(10, 14))
    # Decision: annot=False because a 50x2 matrix is unreadable with numbers.
    sns.heatmap(A_matrix_sorted, cmap='coolwarm', annot=False)
    plt.title('GLS Dispersion Matrix "A" (Benchmark ~ All Stocks)', fontsize=16)
    plt.xlabel('Hedge Instruments')
    plt.ylabel('Portfolio Stocks')
    plt.show()
    
    return A_matrix


# Assume all necessary functions and the 'iv_change_series' DataFrame are defined.

# 1. Define the benchmark assets
benchmarks = ['SPY-UP', 'SPX']

# 2. Calculate the required GLS dispersion matrix and visualize it
A_matrix = calculate_and_plot_gls_dispersion(iv_change_series, benchmarks)

# 3. Define portfolio structure and Vega assumptions
stock_tickers = [t for t in iv_change_series.columns if t not in benchmarks]
hedge_tickers = benchmarks

portfolio_weights = pd.Series(1/len(stock_tickers), index=stock_tickers)
vega_1_percent_stocks = pd.Series(100000.0, index=stock_tickers)
vega_1_percent_hedges = pd.Series({'SPY-UP': 25000.0, 'SPX': 35000.0}, index=hedge_tickers)

# 4. Calculate the optimal hedge using the GLS function and the calculated A_matrix
optimal_hedge = calculate_gls_optimal_hedge(
    portfolio_weights=portfolio_weights,
    vega_1_percent_stocks=vega_1_percent_stocks,
    vega_1_percent_hedges=vega_1_percent_hedges,
    dispersion_matrix=A_matrix,
    iv_change_series=iv_change_series
)

# 5. Display the final result
print("\n--- Final GLS Optimal Hedge ---")
print(optimal_hedge.round(4))

# 16:45 （Time Decay)

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

def get_atm_vol_vs_expiry(surface_df, max_expiry_years=2.0):
    """
    Extracts the ATM volatility term structure from a single volatility surface.

    Args:
        surface_df (pd.DataFrame): The volatility surface for a single day.
        max_expiry_years (float): The maximum time to expiry to consider.

    Returns:
        tuple: A tuple containing arrays for expiries (in years) and
               corresponding ATM implied volatilities.
    """
    # Filter for expiries within the desired range
    surface_df = surface_df[surface_df['Expiry'] <= max_expiry_years].copy()
    if surface_df.empty:
        return np.array([]), np.array([])

    expiries_in_years = surface_df['Expiry'].values
    
    # Extract moneyness levels from column names (e.g., 'm_0_8')
    iv_cols = sorted([c for c in surface_df.columns if c.startswith('m_')],
                     key=lambda c: float(c.replace('m_', '').replace('_', '.')))
    
    moneyness_levels = np.array([float(c.replace('m_', '').replace('_', '.')) for c in iv_cols])
    iv_values = surface_df[iv_cols].values

    # Interpolate to find the ATM (moneyness=1.0) volatility for each expiry
    atm_vols = np.apply_along_axis(
        lambda row: np.interp(1.0, moneyness_levels, row),
        axis=1,
        arr=iv_values
    )
    
    # Remove any NaN or zero values to ensure a clean fit
    valid_indices = ~np.isnan(atm_vols) & (atm_vols > 0) & (expiries_in_years > 0)
    
    return expiries_in_years[valid_indices], atm_vols[valid_indices]


def fit_power_law(expiries, vols):
    """
    Fits a power law (vol = c * T^k) to the volatility term structure.

    The fit is performed using a linear regression on the log-transformed data:
    log(vol) = k * log(T) + log(c).

    Args:
        expiries (np.array): Array of expiries in years.
        vols (np.array): Array of corresponding ATM volatilities.

    Returns:
        tuple: A tuple containing the power law coefficient (k), the R-squared
               value of the fit, and the intercept. Returns (nan, nan, nan) if
               a fit is not possible.
    """
    if len(expiries) < 2:
        return np.nan, np.nan, np.nan

    log_T = np.log(expiries)
    log_v = np.log(vols)

    # Add a constant for the intercept term in the regression
    X = sm.add_constant(log_T)
    model = sm.OLS(log_v, X).fit()
    
    intercept, coefficient = model.params
    r_squared = model.rsquared

    return coefficient, r_squared, intercept

# --- Plotting Total Variance for a Sample Ticker ---
TICKER = 'AAPL-UQ'
DATE = '2023-01-03'

# Load a single surface for demonstration
# Note: The 'load_single_surface' function is from your project description.
# surface, _ = load_single_surface(TICKER, DATE)
# expiries, atm_vols = get_atm_vol_vs_expiry(surface)

# This is a placeholder for the plot as we cannot run the code.
# In a real run, `expiries` and `atm_vols` would be populated.
# total_variance = atm_vols**2 * expiries

# plt.figure(figsize=(10, 6))
# plt.plot(expiries, total_variance, 'o-', color='royalblue')
# plt.title(f'Total Variance (σ²T) vs. Expiry for {TICKER} on {DATE}', fontsize=14)
# plt.xlabel('Time to Expiry (Years)', fontsize=12)
# plt.ylabel('Total Variance (σ²T)', fontsize=12)
# plt.grid(True, linestyle='--', alpha=0.6)
# plt.show()


# --- Plotting Power Law Fit for a Sample Ticker ---
# (Continuing with the same data as above)
# k, r2, _ = fit_power_law(expiries, atm_vols)

# plt.figure(figsize=(10, 6))
# plt.loglog(expiries, atm_vols, 'o', label='Actual Data Points', color='darkorange')
# fit_line = np.exp(_.intercept) * expiries**k
# plt.loglog(expiries, fit_line, '-', label=f'Power Law Fit (k={k:.3f}, R²={r2:.3f})', color='navy')
# plt.title(f'Log-Log Plot of IV vs. Expiry for {TICKER} on {DATE}', fontsize=14)
# plt.xlabel('Log(Time to Expiry)', fontsize=12)
# plt.ylabel('Log(Implied Volatility)', fontsize=12)
# plt.legend()
# plt.grid(True, which="both", ls="--", alpha=0.5)
# plt.show()

# 5:30

import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns

def fit_power_law(expiries, vols):
    """Fits a power law (vol = c * T^k) to the volatility term structure."""
    if len(expiries) < 2:
        return np.nan, np.nan
        
    log_T = np.log(expiries)
    log_v = np.log(vols)
    X = sm.add_constant(log_T)
    model = sm.OLS(log_v, X).fit()
    
    return model.params[1], model.rsquared # coefficient k, r_squared

def calculate_decay_coefficients(tickers, start_date, end_date, db_path='vol_surfaces.h5', max_expiry_years=2.0):
    """
    Calculates the power law decay coefficient (k) for the ATM volatility
    term structure for multiple tickers over a date range.
    """
    all_results = []
    
    with pd.HDFStore(db_path, mode='r') as store:
        all_file_keys = store.keys()
        for ticker in tickers:
            ticker_key_prefix = f"/{ticker.replace('-', '_').replace('.', '')}/"
            keys_for_ticker = [k for k in all_file_keys if k.startswith(ticker_key_prefix)]
            
            for key in keys_for_ticker:
                date = pd.to_datetime(key.split('/')[-1].replace('d', ''), format='%Y%m%d')
                if not (pd.to_datetime(start_date) <= date <= pd.to_datetime(end_date)):
                    continue

                surface_df = store.get(key)
                
                # Process the surface for ATM vol vs. expiry
                surface_df = surface_df[surface_df['Expiry'] <= max_expiry_years].copy()
                
                # Directly use the 'm_1_0' column for ATM vol, no interpolation needed
                atm_vols = surface_df['m_1_0'].values
                expiries = surface_df['Expiry'].values
                
                valid_indices = (atm_vols > 0) & (expiries > 0) & ~np.isnan(atm_vols)
                
                if np.sum(valid_indices) < 2:
                    continue
                    
                expiries_clean = expiries[valid_indices]
                atm_vols_clean = atm_vols[valid_indices]
                
                # Fit the power law to get the decay coefficient
                k, r2 = fit_power_law(expiries_clean, atm_vols_clean)
                
                if not np.isnan(k):
                    all_results.append({'date': date, 'ticker': ticker, 'k': k, 'r_squared': r2})

    return pd.DataFrame(all_results).sort_values(by=['ticker', 'date']).set_index('date')

# Tickers for analysis
TICKERS = ['AAPL-UQ', 'NVDA-UQ', 'TSLA-UQ', 'SPY-UP', 'JPM-UN']
START_DATE = '2023-01-01'
END_DATE = '2023-12-31'

# Generate the results
# decay_df = calculate_decay_coefficients(TICKERS, START_DATE, END_DATE)

# 1:50

import pandas as pd
import numpy as np
from sklearn.linear_model import Ridge
from statsmodels.tsa.stattools import adfuller
from scipy import stats
import matplotlib.pyplot as plt

def calculate_power_law_coefficients(iv_change_series_dict, maturities, tickers, regularization_lambda=5.0):
    results = []
    # Assumes all DataFrames in the dict share the same index
    dates = iv_change_series_dict[list(iv_change_series_dict.keys())[0]].index
    maturity_keys = list(iv_change_series_dict.keys())

    for ticker in tickers:
        for date in dates:
            term_structure_iv_change = [iv_change_series_dict[key].loc[date, ticker] for key in maturity_keys]
            
            iv_changes = np.array(term_structure_iv_change)
            valid_mask = (iv_changes != 0) & ~np.isnan(iv_changes)

            if np.sum(valid_mask) < 2:
                continue
            
            # Using the absolute value of IV change for the log transform is a necessary step
            y = np.log(np.abs(iv_changes[valid_mask]))
            X = np.log(np.array(maturities)[valid_mask]).reshape(-1, 1)

            model = Ridge(alpha=regularization_lambda, fit_intercept=True)
            model.fit(X, y)
            
            k = model.coef_[0]
            results.append({'date': date, 'ticker': ticker, 'k': k})
            
    return pd.DataFrame(results).pivot(index='date', columns='ticker', values='k')

def plot_coefficient_time_series(coefficients_df, benchmarks):
    plt.figure(figsize=(15, 8))
    for ticker in coefficients_df.columns:
        if ticker in benchmarks:
            plt.plot(coefficients_df.index, coefficients_df[ticker], label=ticker, linewidth=2.5, linestyle='-')
        else:
            plt.plot(coefficients_df.index, coefficients_df[ticker], label=ticker, linewidth=1.0, linestyle='--')

    plt.title('Time Series of Power Law Coefficient (k)', fontsize=16)
    plt.xlabel('Date', fontsize=12)
    plt.ylabel('Coefficient k', fontsize=12)
    plt.legend()
    plt.grid(True, which='both', linestyle='--', alpha=0.6)
    plt.show()

def plot_power_law_fit_example(ticker_to_plot, iv_change_series_dict, maturities, coefficients_df):
    maturity_keys = list(iv_change_series_dict.keys())
    avg_iv_changes = []
    for key in maturity_keys:
        # Calculate the average of the absolute change for plotting purposes
        avg_change = np.mean(np.abs(iv_change_series_dict[key][ticker_to_plot]))
        avg_iv_changes.append(avg_change)

    avg_k = coefficients_df[ticker_to_plot].mean()
    # Recalculate the intercept based on the average values
    log_T = np.log(maturities)
    log_abs_delta_sigma = np.log(avg_iv_changes)
    avg_intercept = np.mean(log_abs_delta_sigma - avg_k * log_T)
    
    plt.figure(figsize=(10, 6))
    plt.loglog(maturities, avg_iv_changes, 'o', label=f'Avg |Δσ| for {ticker_to_plot}')
    
    fit_line = np.exp(avg_intercept) * (np.array(maturities) ** avg_k)
    plt.loglog(maturities, fit_line, '-', label=f'Fitted Power Law (Avg k={avg_k:.3f})')
    
    plt.title(f'Power Law Fit for {ticker_to_plot}', fontsize=16)
    plt.xlabel('Maturity (Years) - Log Scale', fontsize=12)
    plt.ylabel('Average |Δσ| - Log Scale', fontsize=12)
    plt.legend()
    plt.grid(True, which='both', linestyle='--', alpha=0.5)
    plt.show()

def analyze_coefficient_stability(coefficients_df):
    stability_results = []
    for ticker in coefficients_df.columns:
        series = coefficients_df[ticker].dropna()
        
        mean_k = series.mean()
        std_k = series.std()
        
        # Drift calculated as the slope of k vs. time (in years)
        time_in_years = (series.index - series.index[0]).days / 365.25
        drift = np.polyfit(time_in_years, series, 1)[0]
        
        adf_result = adfuller(series)
        
        stability_results.append({
            'Ticker': ticker,
            'Mean (k)': mean_k,
            'Std Dev': std_k,
            'Drift (per year)': drift,
            'ADF p-value': adf_result[1]
        })
        
    return pd.DataFrame(stability_results)

def analyze_total_variance_stability(iv_change_series_dict, maturities, tickers):
    all_slopes = {}
    dates = iv_change_series_dict[list(iv_change_series_dict.keys())[0]].index
    maturity_keys = list(iv_change_series_dict.keys())
    
    for ticker in tickers:
        daily_slopes = []
        for date in dates:
            term_structure_iv_change = [iv_change_series_dict[key].loc[date, ticker] for key in maturity_keys]
            
            iv_changes = np.array(term_structure_iv_change)
            total_variance_change = iv_changes**2 * np.array(maturities)
            
            valid_mask = ~np.isnan(total_variance_change)
            if np.sum(valid_mask) < 2:
                daily_slopes.append(np.nan)
                continue

            # Regress (Δσ)²T against T to find the slope
            slope = np.polyfit(np.array(maturities)[valid_mask], total_variance_change[valid_mask], 1)[0]
            daily_slopes.append(slope)
            
        all_slopes[ticker] = daily_slopes
        
    return pd.DataFrame(all_slopes, index=dates)

def test_total_variance_slopes(slopes_df):
    test_results = []
    for ticker in slopes_df.columns:
        series = slopes_df[ticker].dropna()
        
        mean_slope = series.mean()
        ttest_result = stats.ttest_1samp(series, 0)
        
        test_results.append({
            'Ticker': ticker,
            'Mean Slope': mean_slope,
            't-statistic': ttest_result.statistic,
            'p-value': ttest_result.pvalue
        })
        
    return pd.DataFrame(test_results)
